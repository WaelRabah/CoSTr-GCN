{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d742038",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wrabah-externe\\Anaconda3\\envs\\alt_env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "import continual as co\n",
    "import pytorch_lightning as pl\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from abc import abstractmethod\n",
    "from typing_extensions import Self\n",
    "import math\n",
    "from typing import  Tuple, Optional, Any, Union\n",
    "from functools import partial\n",
    "from continual import TensorPlaceholder\n",
    "from continual.module import CallMode\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c1f1577",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "\n",
    "def edge2mat(link, num_node):\n",
    "    A = np.zeros((num_node, num_node))\n",
    "    for i, j in link:\n",
    "        A[j, i] = 1\n",
    "    return A\n",
    "\n",
    "\n",
    "def normalize_digraph(A):\n",
    "    Dl = np.sum(A, 0)\n",
    "    _, w = A.shape\n",
    "    Dn = np.zeros((w, w))\n",
    "    for i in range(w):\n",
    "        if Dl[i] > 0:\n",
    "            Dn[i, i] = Dl[i] ** (-1)\n",
    "    AD = np.dot(A, Dn)\n",
    "    return AD\n",
    "\n",
    "\n",
    "def get_spatial_graph(num_node, self_link, inward, outward):\n",
    "    I = edge2mat(self_link, num_node)  # noqa: E741\n",
    "    In = normalize_digraph(edge2mat(inward, num_node))\n",
    "    Out = normalize_digraph(edge2mat(outward, num_node))\n",
    "    A = np.stack((I, In, Out))\n",
    "    return A\n",
    "\n",
    "\n",
    "class Graph():\n",
    "\n",
    "    def __init__(self,\n",
    "                 layout='DHG14/28',\n",
    "                 strategy='uniform',\n",
    "                 max_hop=2,\n",
    "                 dilation=1):\n",
    "        self.max_hop = max_hop\n",
    "        self.dilation = dilation\n",
    "\n",
    "        self.get_edge(layout)\n",
    "        self.hop_dis = self.get_hop_distance(\n",
    "            self.num_node, self.edge, max_hop=max_hop)\n",
    "        self.get_adjacency(strategy)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.A\n",
    "\n",
    "    def get_edge(self, layout):\n",
    "        if layout == 'DHG14/28':\n",
    "            self.num_node = 22\n",
    "            self_link = [(i, i) for i in range(self.num_node)]\n",
    "            neighbor_link = [(0, 1),\n",
    "                             (0, 2),\n",
    "                             (1, 0),\n",
    "                             (1, 6),\n",
    "                             (1, 10),\n",
    "                             (1, 14),\n",
    "                             (1, 18),\n",
    "                             (2, 0),\n",
    "                             (2, 3),\n",
    "                             (3, 2),\n",
    "                             (3, 4),\n",
    "                             (4, 3),\n",
    "                             (4, 5),\n",
    "                             (5, 4),\n",
    "                             (6, 1),\n",
    "                             (6, 7),\n",
    "                             (7, 6),\n",
    "                             (7, 8),\n",
    "                             (8, 7),\n",
    "                             (8, 9),\n",
    "                             (9, 8),\n",
    "                             (10, 1),\n",
    "                             (10, 11),\n",
    "                             (11, 10),\n",
    "                             (11, 12),\n",
    "                             (12, 11),\n",
    "                             (12, 13),\n",
    "                             (13, 12),\n",
    "                             (14, 1),\n",
    "                             (14, 15),\n",
    "                             (15, 14),\n",
    "                             (15, 16),\n",
    "                             (16, 15),\n",
    "                             (16, 17),\n",
    "                             (17, 16),\n",
    "                             (18, 1),\n",
    "                             (18, 19),\n",
    "                             (19, 18),\n",
    "                             (19, 20),\n",
    "                             (20, 19),\n",
    "                             (20, 21),\n",
    "                             (21, 20)]\n",
    "            self.edge = self_link + neighbor_link\n",
    "            self.center = 1\n",
    "        elif layout == \"SHREC21\":\n",
    "            self.num_node = 20\n",
    "            self_link = [(i, i) for i in range(self.num_node)]\n",
    "            neighbor_link = [\n",
    "                (0, 1),\n",
    "                (1, 2),\n",
    "                (2, 3),\n",
    "                (0, 4),\n",
    "                (4, 5),\n",
    "                (5, 6),\n",
    "                (6, 7),\n",
    "                (0, 8),\n",
    "                (8, 9),\n",
    "                (9, 10),\n",
    "                (10, 11),\n",
    "                (0, 12),\n",
    "                (12, 13),\n",
    "                (13, 14),\n",
    "                (14, 15),\n",
    "                (0, 16),\n",
    "                (16, 17),\n",
    "                (17, 18),\n",
    "                (18, 19),\n",
    "            ]\n",
    "            self.edge = self_link + neighbor_link\n",
    "            self.center = 0\n",
    "        elif layout == \"FPHA\":\n",
    "            self.num_node = 21\n",
    "            self_link = [(i, i) for i in range(self.num_node)]\n",
    "            neighbor_link = [\n",
    "                (0, 1),\n",
    "                (0, 2),\n",
    "                (0, 3),\n",
    "                (0, 4),\n",
    "                (0, 5),\n",
    "                (1, 0),\n",
    "                (1, 6),\n",
    "                (2, 0),\n",
    "                (2, 7),\n",
    "                (3, 0),\n",
    "                (3, 8),\n",
    "                (4, 0),\n",
    "                (4, 9),\n",
    "                (5, 0),\n",
    "                (5, 10),\n",
    "                (6, 1),\n",
    "                (6, 11),\n",
    "                (7, 2),\n",
    "                (7, 12),\n",
    "                (8, 3),\n",
    "                (8, 13),\n",
    "                (9, 4),\n",
    "                (9, 14),\n",
    "                (10, 5),\n",
    "                (10, 15),\n",
    "                (11, 6),\n",
    "                (11, 16),\n",
    "                (12, 7),\n",
    "                (12, 17),\n",
    "                (13, 8),\n",
    "                (13, 18),\n",
    "                (14, 9),\n",
    "                (14, 19),\n",
    "                (15, 10),\n",
    "                (15, 20),\n",
    "                (16, 11),\n",
    "                (17, 12),\n",
    "                (18, 13),\n",
    "                (19, 14),\n",
    "                (20, 15)\n",
    "            ]\n",
    "            self.edge = self_link + neighbor_link\n",
    "            self.center = 0\n",
    "        else:\n",
    "            raise ValueError(\"Do Not Exist This Layout.\")\n",
    "\n",
    "    def get_adjacency(self, strategy):\n",
    "        valid_hop = range(0, self.max_hop + 1, self.dilation)\n",
    "        adjacency = np.zeros((self.num_node, self.num_node))\n",
    "        for hop in valid_hop:\n",
    "            adjacency[self.hop_dis == hop] = 1\n",
    "        normalize_adjacency = self.normalize_digraph(adjacency)\n",
    "\n",
    "        if strategy == 'uniform':\n",
    "            A = np.zeros((1, self.num_node, self.num_node))\n",
    "            A[0] = normalize_adjacency\n",
    "            self.A = A\n",
    "        elif strategy == 'distance':\n",
    "            A = np.zeros((len(valid_hop), self.num_node, self.num_node))\n",
    "            for i, hop in enumerate(valid_hop):\n",
    "                A[i][self.hop_dis == hop] = normalize_adjacency[self.hop_dis == hop]\n",
    "            self.A = A\n",
    "        elif strategy == 'spatial':\n",
    "            A = []\n",
    "            for hop in valid_hop:\n",
    "                a_root = np.zeros((self.num_node, self.num_node))\n",
    "                a_close = np.zeros((self.num_node, self.num_node))\n",
    "                a_further = np.zeros((self.num_node, self.num_node))\n",
    "                for i in range(self.num_node):\n",
    "                    for j in range(self.num_node):\n",
    "                        if self.hop_dis[j, i] == hop:\n",
    "                            if self.hop_dis[j, self.center] == self.hop_dis[i, self.center]:\n",
    "                                a_root[j, i] = normalize_adjacency[j, i]\n",
    "                            elif self.hop_dis[j, self.center] > self.hop_dis[i, self.center]:\n",
    "                                a_close[j, i] = normalize_adjacency[j, i]\n",
    "                            else:\n",
    "                                a_further[j, i] = normalize_adjacency[j, i]\n",
    "                if hop == 0:\n",
    "                    A.append(a_root)\n",
    "                else:\n",
    "                    A.append(a_root + a_close)\n",
    "                    A.append(a_further)\n",
    "            A = np.stack(A)\n",
    "            self.A = A\n",
    "        else:\n",
    "            raise ValueError(\"Do Not Exist This Strategy\")\n",
    "\n",
    "    def get_hop_distance(self, num_node, edge, max_hop=1):\n",
    "        A = np.zeros((num_node, num_node))\n",
    "        for i, j in edge:\n",
    "            A[j, i] = 1\n",
    "            A[i, j] = 1\n",
    "\n",
    "        hop_dis = np.zeros((num_node, num_node)) + np.inf\n",
    "        transfer_mat = [np.linalg.matrix_power(\n",
    "            A, d) for d in range(max_hop + 1)]\n",
    "        arrive_mat = (np.stack(transfer_mat) > 0)\n",
    "        for d in range(max_hop, -1, -1):\n",
    "            hop_dis[arrive_mat[d]] = d\n",
    "        return hop_dis\n",
    "\n",
    "    def normalize_digraph(self, A):\n",
    "        Dl = np.sum(A, 0)\n",
    "        num_node = A.shape[0]\n",
    "        Dn = np.zeros((num_node, num_node))\n",
    "        for i in range(num_node):\n",
    "            if Dl[i] > 0:\n",
    "                Dn[i, i] = Dl[i]**(-1)\n",
    "        AD = np.dot(A, Dn)\n",
    "        return AD\n",
    "\n",
    "    def normalize_undigraph(self, A):\n",
    "        Dl = np.sum(A, 0)\n",
    "        num_node = A.shape[0]\n",
    "        Dn = np.zeros((num_node, num_node))\n",
    "        for i in range(num_node):\n",
    "            if Dl[i] > 0:\n",
    "                Dn[i, i] = Dl[i]**(-0.5)\n",
    "        DAD = np.dot(np.dot(Dn, A), Dn)\n",
    "        return DAD\n",
    "\n",
    "\n",
    "num_joint = 20\n",
    "max_frame = 2500\n",
    "\n",
    "\n",
    "class Feeder_SHREC21(Dataset):\n",
    "    \"\"\"\n",
    "    Feeder for skeleton-based gesture recognition in shrec21-skeleton dataset\n",
    "    Arguments:\n",
    "        data_path: the path to '.npy' data, the shape of data should be (N, C, T, V, M)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            data_path=\"SHREC21\",\n",
    "            set_name=\"training\",\n",
    "            window_size=10,\n",
    "            aug_by_sw=False,\n",
    "            is_segmented=False\n",
    "    ):\n",
    "        self.data_path = data_path\n",
    "        self.set_name = set_name\n",
    "        self.classes = [\"\",\n",
    "                        \"RIGHT\",\n",
    "                        \"KNOB\",\n",
    "                        \"CROSS\",\n",
    "                        \"THREE\",\n",
    "                        \"V\",\n",
    "                        \"ONE\",\n",
    "                        \"FOUR\",\n",
    "                        \"GRAB\",\n",
    "                        \"DENY\",\n",
    "                        \"MENU\",\n",
    "                        \"CIRCLE\",\n",
    "                        \"TAP\",\n",
    "                        \"PINCH\",\n",
    "                        \"LEFT\",\n",
    "                        \"TWO\",\n",
    "                        \"OK\",\n",
    "                        \"EXPAND\",\n",
    "                        ]\n",
    "        self.class_to_idx = {class_l: idx for idx,\n",
    "                             class_l in enumerate(self.classes)}\n",
    "        self.window_size = window_size\n",
    "        self.aug_by_sw = aug_by_sw\n",
    "        self.is_segmented = is_segmented\n",
    "        self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        self.dataset = []\n",
    "        # load file list\n",
    "        # classes = set([''])\n",
    "        # self.classes = []\n",
    "        with open(\n",
    "                f'{self.data_path}/{self.set_name}_set/annotations_revised.txt' if self.set_name == \"test\" else f'{self.data_path}/{self.set_name}_set/annotations_revised_{self.set_name}.txt',\n",
    "                mode=\"r\") as f:\n",
    "\n",
    "            for line in f:\n",
    "                fields = line.split(';')\n",
    "                seq_idx = fields[0]\n",
    "                gestures = fields[1:-1]\n",
    "                nb_gestures = len(gestures) // 3\n",
    "                gesture_infos = []\n",
    "                for i in range(nb_gestures):\n",
    "                    gesture_info = gestures[i * 3:(i + 1) * 3]\n",
    "                    gesture_label = gesture_info[0]\n",
    "                    gesture_start = gesture_info[1]\n",
    "                    gesture_end = gesture_info[2]\n",
    "                    gesture_infos.append(\n",
    "                        (gesture_start, gesture_end, gesture_label))\n",
    "                    # classes.add(gesture_label)\n",
    "                self.dataset.append((seq_idx, gesture_infos))\n",
    "\n",
    "        # self.classes = list(classes)\n",
    "        # with open('datasets/shrec21/classes.yaml', mode=\"w\") as f:\n",
    "        #     yaml.dump(self.classes, f, explicit_start=True, default_flow_style=False)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        def parse_seq_data(src_file):\n",
    "            '''\n",
    "            Retrieves the skeletons sequence for each gesture\n",
    "            '''\n",
    "            video = []\n",
    "            mode = \"pos\"\n",
    "            for line in src_file:\n",
    "\n",
    "                line = line.split(\"\\n\")[0]\n",
    "\n",
    "                data = line.split(\";\")[:-1]\n",
    "\n",
    "                frame = []\n",
    "                point = []\n",
    "                for data_ele in data:\n",
    "                    if len(data_ele) == 0:\n",
    "                        continue\n",
    "                    point.append(float(data_ele))\n",
    "\n",
    "                    if len(point) == 3 and mode == \"pos\":\n",
    "                        frame.append(point)\n",
    "                        point = []\n",
    "                        mode = \"quat\"\n",
    "                    elif len(point) == 4 and mode == \"quat\":\n",
    "                        frame.append(point)\n",
    "                        point = []\n",
    "                        mode = \"pos\"\n",
    "                if len(frame) > 0:\n",
    "                    positions = []\n",
    "                    quats = []\n",
    "\n",
    "                    for i in range(num_joint):\n",
    "                        positions.append(frame[i*2])\n",
    "                        quats.append(frame[i*2+1])\n",
    "\n",
    "                    video.append(positions)\n",
    "            return np.array(video)\n",
    "\n",
    "        def sample_window(data_num, stride):\n",
    "            # sample #window_size frames from whole video\n",
    "\n",
    "            sample_size = self.window_size\n",
    "\n",
    "            idx_list = [0, data_num - 1]\n",
    "            for i in range(sample_size):\n",
    "                if index not in idx_list and index < data_num:\n",
    "                    idx_list.append(index)\n",
    "            idx_list.sort()\n",
    "\n",
    "            while len(idx_list) < sample_size:\n",
    "                idx = random.randint(0, data_num - 1)\n",
    "                if idx not in idx_list:\n",
    "                    idx_list.append(idx)\n",
    "            idx_list.sort()\n",
    "            return idx_list\n",
    "        # output shape (C, T, V, M)\n",
    "        # get data\n",
    "\n",
    "        def get_segmented(seq_idx, gesture_infos):\n",
    "            with open(f'{self.data_path}/{self.set_name}_set/sequences/{seq_idx}.txt', mode=\"r\") as seq_f:\n",
    "                sequence = parse_seq_data(seq_f)\n",
    "            labeled_sequence = [(f, \"\") for f in sequence]\n",
    "            # if len(labeled_sequence) > max_frame:\n",
    "            #     max_frame = len(labeled_sequence)\n",
    "            for gesture_start, gesture_end, gesture_label in gesture_infos:\n",
    "                labeled_sequence = [\n",
    "                    (np.array(f), gesture_label if int(gesture_start) <=\n",
    "                     idx <= int(gesture_end) and label == \"\" else label)\n",
    "                    for\n",
    "                    idx, (f, label) in enumerate(labeled_sequence)]\n",
    "\n",
    "            frames = [f for f, l in labeled_sequence]\n",
    "            # print(len(self.classes))\n",
    "            labels_per_frame = [self.class_to_idx[l]\n",
    "                                for f, l in labeled_sequence]\n",
    "            gestures = []\n",
    "            windows_sub_sequences_per_gesture = {\n",
    "                i: [] for i in range(len(self.classes))}\n",
    "\n",
    "            for gesture_start, gesture_end, gesture_label in gesture_infos:\n",
    "                gesture_start = int(gesture_start)\n",
    "                gesture_end = int(gesture_end)\n",
    "                g_frames = frames[gesture_start:gesture_end]\n",
    "                g_label = labels_per_frame[gesture_start:gesture_end]\n",
    "                gestures.append((g_frames, g_label))\n",
    "                label = self.class_to_idx[gesture_label]\n",
    "                if self.aug_by_sw:\n",
    "                    num_windows = len(g_frames) // self.window_size\n",
    "\n",
    "                    for stride in range(1, self.window_size):\n",
    "                        l = len(g_frames)\n",
    "                        if l // stride >= self.window_size:\n",
    "                            window_indices = sample_window(l, stride)\n",
    "                            window = [g_frames[idx] for idx in window_indices]\n",
    "                            windows_sub_sequences_per_gesture[label].append(\n",
    "                                (window, label))\n",
    "\n",
    "            ng_sequences = []\n",
    "            ng_seq = []\n",
    "            l = len(frames)\n",
    "            indices_ng = []\n",
    "            for i in range(len(frames)-1):\n",
    "                f_curr = frames[i]\n",
    "                f_next = frames[i+1]\n",
    "                l_curr = labels_per_frame[i]\n",
    "                l_next = labels_per_frame[i+1]\n",
    "\n",
    "                if l_curr == 0 and l_next == 0:\n",
    "                    indices_ng.append(i)\n",
    "                    ng_seq.append(f_curr)\n",
    "                    if i == l-2:\n",
    "                        ng_seq.append(f_next)\n",
    "                        ng_sequences.append((ng_seq, 0))\n",
    "                        ng_seq = []\n",
    "                        continue\n",
    "                elif l_curr == 0 and l_next != 0:\n",
    "                    indices_ng.append(i)\n",
    "                    ng_seq.append(f_curr)\n",
    "                    ng_sequences.append((ng_seq, 0))\n",
    "                    ng_seq = []\n",
    "                    continue\n",
    "\n",
    "            return gestures, ng_sequences, windows_sub_sequences_per_gesture\n",
    "\n",
    "        def get_full_sequences(seq_idx, gesture_infos):\n",
    "            with open(f'{self.data_path}/{self.set_name}_set/sequences/{seq_idx}.txt', mode=\"r\") as seq_f:\n",
    "                sequence = parse_seq_data(seq_f)\n",
    "            labeled_sequence = [(f, \"\") for f in sequence]\n",
    "            # if len(labeled_sequence) > max_frame:\n",
    "            #     max_frame = len(labeled_sequence)\n",
    "            for gesture_start, gesture_end, gesture_label in gesture_infos:\n",
    "                labeled_sequence = [\n",
    "                    (np.array(f), gesture_label if int(gesture_start) <=\n",
    "                     idx <= int(gesture_end) and label == \"\" else label)\n",
    "                    for\n",
    "                    idx, (f, label) in enumerate(labeled_sequence)]\n",
    "\n",
    "            frames = [f for f, l in labeled_sequence]\n",
    "\n",
    "            labels_per_frame = [self.classes.index(\n",
    "                l) for f, l in labeled_sequence]\n",
    "            return labeled_sequence, np.array(frames), labels_per_frame\n",
    "        seq_idx, gesture_infos = self.dataset[index]\n",
    "\n",
    "        if self.is_segmented:\n",
    "            return get_segmented(seq_idx, gesture_infos)\n",
    "        else:\n",
    "            return get_full_sequences(seq_idx, gesture_infos)\n",
    "\n",
    "\n",
    "def get_window_label(label, num_classes=18):\n",
    "\n",
    "    W = len(label)\n",
    "    sum = torch.zeros((num_classes))\n",
    "    for t in range(W):\n",
    "        sum[label[t]] += 1\n",
    "    return sum.argmax(dim=-1).item()\n",
    "\n",
    "\n",
    "def gendata(\n",
    "        data_path,\n",
    "        set_name,\n",
    "        max_frame,\n",
    "        window_size=20,\n",
    "        aug_by_sw=False,\n",
    "        is_segmented=False\n",
    "):\n",
    "    feeder = Feeder_SHREC21(\n",
    "        data_path=data_path,\n",
    "        set_name=set_name,\n",
    "        window_size=window_size,\n",
    "        aug_by_sw=aug_by_sw,\n",
    "        is_segmented=is_segmented\n",
    "    )\n",
    "    dataset = feeder.dataset\n",
    "    if is_segmented:\n",
    "        data = []\n",
    "        ng_sequences_data = []\n",
    "        windows_sub_sequences_data = {i: []\n",
    "                                      for i in range(len(feeder.classes))}\n",
    "        for i, s in enumerate(tqdm(dataset)):\n",
    "            data_el, ng_sequences, windows_sub_sequences_per_gesture = feeder[i]\n",
    "            ng_sequences_data = [*ng_sequences_data, *ng_sequences]\n",
    "            l = len(data_el)\n",
    "            # for w in range(num_windows):\n",
    "            for idx, gesture in enumerate(data_el):\n",
    "                current_skeletons_window = np.array(gesture[0])\n",
    "                label = gesture[1]\n",
    "                label = get_window_label(label)\n",
    "                windows_sub_sequences_data[label] = [\n",
    "                    *windows_sub_sequences_data[label], *windows_sub_sequences_per_gesture[label]]\n",
    "                data.append((current_skeletons_window, label))\n",
    "\n",
    "        return data, ng_sequences_data, windows_sub_sequences_data\n",
    "    else:\n",
    "        data = []\n",
    "        for i, s in enumerate(tqdm(dataset)):\n",
    "            labeled_seq, frames, labels = feeder[i]\n",
    "            data.append((frames, labels))\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "class GraphDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path,\n",
    "        set_name,\n",
    "        window_size,\n",
    "        use_data_aug=False,\n",
    "        normalize=True,\n",
    "        scaleInvariance=False,\n",
    "        translationInvariance=False,\n",
    "        isPadding=False,\n",
    "        useSequenceFragments=False,\n",
    "        useRandomMoving=False,\n",
    "        useMirroring=False,\n",
    "        useTimeInterpolation=False,\n",
    "        useNoise=False,\n",
    "        useScaleAug=False,\n",
    "        useTranslationAug=False,\n",
    "        use_aug_by_sw=False,\n",
    "        nb_sub_sequences=10,\n",
    "        sample_classes=False,\n",
    "        is_segmented=False,\n",
    "        number_of_samples_per_class=0\n",
    "    ):\n",
    "        \"\"\"Initialise a Graph dataset\n",
    "        \"\"\"\n",
    "        self.data_path = data_path\n",
    "        self.set_name = set_name\n",
    "        self.use_data_aug = use_data_aug\n",
    "        self.window_size = window_size\n",
    "        self.compoent_num = 20\n",
    "        self.normalize = normalize\n",
    "        self.scaleInvariance = scaleInvariance\n",
    "        self.translationInvariance = translationInvariance\n",
    "        # self.transform = transform\n",
    "        self.isPadding = isPadding\n",
    "        self.useSequenceFragments = useSequenceFragments\n",
    "        self.useRandomMoving = useRandomMoving\n",
    "        self.useMirroring = useMirroring\n",
    "        self.useTimeInterpolation = useTimeInterpolation\n",
    "        self.useNoise = useNoise\n",
    "        self.useScaleAug = useScaleAug\n",
    "        self.useTranslationAug = useTranslationAug\n",
    "        self.use_aug_by_sw = use_aug_by_sw\n",
    "        self.number_of_samples_per_class = number_of_samples_per_class\n",
    "        self.is_segmented = is_segmented\n",
    "        self.nb_sub_sequences=nb_sub_sequences\n",
    "        self.sample_classes_=sample_classes\n",
    "        self.classes = [\"No gesture\",\n",
    "                        \"RIGHT\",\n",
    "                        \"KNOB\",\n",
    "                        \"CROSS\",\n",
    "                        \"THREE\",\n",
    "                        \"V\",\n",
    "                        \"ONE\",\n",
    "                        \"FOUR\",\n",
    "                        \"GRAB\",\n",
    "                        \"DENY\",\n",
    "                        \"MENU\",\n",
    "                        \"CIRCLE\",\n",
    "                        \"TAP\",\n",
    "                        \"PINCH\",\n",
    "                        \"LEFT\",\n",
    "                        \"TWO\",\n",
    "                        \"OK\",\n",
    "                        \"EXPAND\",\n",
    "                        ]\n",
    "        self.load_data()\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def load_data(self):\n",
    "        # Data: N C V T M\n",
    "        if self.is_segmented :\n",
    "            self.data, self.ng_sequences_data, self.gesture_sub_sequences_data = gendata(\n",
    "                self.data_path,\n",
    "                self.set_name,\n",
    "                max_frame,\n",
    "                self.window_size,\n",
    "                self.use_aug_by_sw,\n",
    "                self.is_segmented\n",
    "            )\n",
    "            self.sample_no_gesture_class()\n",
    "            print(\"Number of gestures per class in the original \"+self.set_name+\" set :\")\n",
    "            self.print_classes_information()\n",
    "            print(self.set_name)\n",
    "            data = []\n",
    "            for idx, data_el in enumerate(self.data):\n",
    "                if np.array(data_el[0]).shape[0] > 0:\n",
    "                    data.append(data_el)\n",
    "\n",
    "            self.data = data\n",
    "            if self.sample_classes_:\n",
    "                self.sample_classes(self.nb_sub_sequences)\n",
    "            if self.use_data_aug:\n",
    "                print(\"Augmenting data ....\")\n",
    "                augmented_data = []\n",
    "                for idx, data_el in enumerate(self.data):\n",
    "                    augmented_skeletons = self.data_aug(self.preprocessSkeleton(\n",
    "                        torch.from_numpy(np.array(data_el[0])).float()))\n",
    "                    for s in augmented_skeletons:\n",
    "                        augmented_data.append((s, data_el[1]))\n",
    "                self.data = augmented_data\n",
    "            if self.use_aug_by_sw or self.use_data_aug:\n",
    "                print(\"Number of gestures per class in the \" +\n",
    "                    self.set_name+\" set after augmentation:\")\n",
    "                self.print_classes_information()\n",
    "        else :\n",
    "            self.data = gendata(\n",
    "                self.data_path,\n",
    "                self.set_name,\n",
    "                max_frame,\n",
    "                self.window_size,\n",
    "                self.use_aug_by_sw,\n",
    "                self.is_segmented\n",
    "            )\n",
    "\n",
    "    def print_classes_information(self):\n",
    "        data_dict = {i: 0 for i in range(len(self.classes))}\n",
    "        for seq, label in self.data:\n",
    "            data_dict[label] += 1\n",
    "        for class_label in data_dict.keys():\n",
    "            print(\"Class\", self.classes[class_label],\n",
    "                  \"has\", data_dict[class_label], \"samples\")\n",
    "\n",
    "    def sample_no_gesture_class(self):\n",
    "        random.Random(4).shuffle(self.ng_sequences_data)\n",
    "        print(len(self.ng_sequences_data))\n",
    "        samples = self.ng_sequences_data[:self.number_of_samples_per_class*2+(self.nb_sub_sequences if self.use_aug_by_sw else 0)]\n",
    "\n",
    "        self.data = [*self.data, *samples]\n",
    "\n",
    "    def sample_classes(self, nb_sub_sequences):\n",
    "        # Data: N C V T M\n",
    "        data_dict = {i: [] for i in range(len(self.classes))}\n",
    "        data = []\n",
    "        for seq, label in self.data:\n",
    "            data_dict[label].append((seq, label))\n",
    "\n",
    "        for k in data_dict.keys():\n",
    "            samples = data_dict[k][:self.number_of_samples_per_class if k !=\n",
    "                                   0 else self.number_of_samples_per_class * 3]\n",
    "            if self.use_aug_by_sw:\n",
    "                samples = [\n",
    "                    *samples, *self.gesture_sub_sequences_data[k][:nb_sub_sequences]]\n",
    "            data = [*data, *samples]\n",
    "\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def preprocessSkeleton(self, skeleton):\n",
    "        def translationInvariance(skeleton):\n",
    "            # normalize by palm center value at frame=1\n",
    "            skeleton -= torch.clone(skeleton[0][1])\n",
    "            skeleton = skeleton.float()\n",
    "            return skeleton\n",
    "\n",
    "        def scaleInvariance(skeleton):\n",
    "\n",
    "            x_c = torch.clone(skeleton)\n",
    "\n",
    "            distance = torch.sqrt(torch.sum((x_c[0, 1]-x_c[0, 0])**2, dim=-1))\n",
    "\n",
    "            factor = 1/distance\n",
    "\n",
    "            x_c *= factor\n",
    "\n",
    "            return x_c\n",
    "\n",
    "        def normalize(skeleton):\n",
    "\n",
    "            # if self.transform:\n",
    "            #     skeleton = self.transform(skeleton.numpy())\n",
    "            skeleton = F.normalize(skeleton)\n",
    "\n",
    "            return skeleton\n",
    "        if self.normalize:\n",
    "            skeleton = normalize(skeleton)\n",
    "        if self.scaleInvariance:\n",
    "            skeleton = scaleInvariance(skeleton)\n",
    "        if self.translationInvariance:\n",
    "            skeleton = translationInvariance(skeleton)\n",
    "\n",
    "        return skeleton\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        data_numpy, label = self.data[index]\n",
    "        # label = self.labels[index]\n",
    "\n",
    "        skeleton = np.array(data_numpy)\n",
    "        \n",
    "        # if self.data_aug :\n",
    "        #     pass\n",
    "\n",
    "        data_num = skeleton.shape[0]\n",
    "        if self.is_segmented==False :\n",
    "            if data_num < max_frame :\n",
    "                if self.isPadding:\n",
    "                    # padding\n",
    "                    skeleton = self.auto_padding(skeleton, max_frame)\n",
    "                    # label\n",
    "                    label=[*label,*[ 0 for _ in range(max_frame-len(label))]]\n",
    "                else :\n",
    "                    skeleton = self.upsample(skeleton, self.window_size)\n",
    "            else :\n",
    "                idx_list = self.sample_frames(data_num, max_frame)\n",
    "                skeleton = [skeleton[idx] for idx in idx_list]\n",
    "                skeleton = np.array(skeleton)\n",
    "                skeleton = torch.from_numpy(skeleton)\n",
    "\n",
    "            return skeleton, label, index\n",
    "            \n",
    "        if data_num >= self.window_size:\n",
    "            idx_list = self.sample_frames(data_num, self.window_size)\n",
    "            skeleton = [skeleton[idx] for idx in idx_list]\n",
    "            skeleton = np.array(skeleton)\n",
    "            skeleton = torch.from_numpy(skeleton)\n",
    "        else:\n",
    "            skeleton = self.upsample(skeleton, self.window_size)\n",
    "\n",
    "        # print(label)\n",
    "        return skeleton, label, index\n",
    "\n",
    "    def data_aug(self, skeleton):\n",
    "\n",
    "        def scale(skeleton):\n",
    "            ratio = 0.2\n",
    "            low = 1 - ratio\n",
    "            high = 1 + ratio\n",
    "            factor = np.random.uniform(low, high)\n",
    "            video_len = skeleton.shape[0]\n",
    "            for t in range(video_len):\n",
    "                for j_id in range(self.compoent_num):\n",
    "                    skeleton[t][j_id] *= factor\n",
    "            skeleton = np.array(skeleton)\n",
    "            return skeleton\n",
    "\n",
    "        def shift(skeleton):\n",
    "            low = -0.1\n",
    "            high = -low\n",
    "            offset = np.random.uniform(low, high, 3)\n",
    "            video_len = skeleton.shape[0]\n",
    "            for t in range(video_len):\n",
    "                for j_id in range(self.compoent_num):\n",
    "                    skeleton[t][j_id] += offset\n",
    "            skeleton = np.array(skeleton)\n",
    "            return skeleton\n",
    "\n",
    "        def noise(skeleton):\n",
    "            low = -0.1\n",
    "            high = -low\n",
    "            # select 4 joints\n",
    "            all_joint = list(range(self.compoent_num))\n",
    "            random.Random(4).shuffle(all_joint)\n",
    "            selected_joint = all_joint[0:4]\n",
    "            for j_id in selected_joint:\n",
    "                noise_offset = np.random.uniform(low, high, 3)\n",
    "                for t in range(skeleton.shape[0]):\n",
    "                    skeleton[t][j_id] += noise_offset\n",
    "\n",
    "            skeleton = np.array(skeleton)\n",
    "            return skeleton\n",
    "\n",
    "        def time_interpolate(skeleton):\n",
    "            skeleton = np.array(skeleton)\n",
    "            video_len = skeleton.shape[0]\n",
    "\n",
    "            r = np.random.uniform(0, 1)\n",
    "\n",
    "            result = []\n",
    "\n",
    "            for i in range(1, video_len):\n",
    "                displace = skeleton[i] - skeleton[i - 1]  # d_t = s_t+1 - s_t\n",
    "                displace *= r\n",
    "                result.append(skeleton[i - 1] + displace)  # r*disp\n",
    "\n",
    "            while len(result) < self.window_size:\n",
    "                result.append(result[-1])  # padding\n",
    "            result = np.array(result)\n",
    "            return result\n",
    "\n",
    "        def random_sequence_fragments(sample):\n",
    "            samples = [sample]\n",
    "            sample = torch.from_numpy(sample)\n",
    "            n_fragments = 5\n",
    "            T, V, C = sample.shape\n",
    "            if T <= self.window_size:\n",
    "                return samples\n",
    "            for _ in range(n_fragments):\n",
    "\n",
    "                # fragment_len=int(T*fragment_len)\n",
    "                fragment_len = self.window_size\n",
    "                max_start_frame = T-fragment_len\n",
    "\n",
    "                random_start_frame = random.randint(0, max_start_frame)\n",
    "                new_sample = sample[random_start_frame:random_start_frame+fragment_len]\n",
    "                samples.append(new_sample.numpy())\n",
    "\n",
    "            return samples\n",
    "\n",
    "        def mirroring(data_numpy):\n",
    "            T, V, C = data_numpy.shape\n",
    "            data_numpy[:, :, 0] = np.max(\n",
    "                data_numpy[:, :, 0]) + np.min(data_numpy[:, :, 0]) - data_numpy[:, :, 0]\n",
    "            return data_numpy\n",
    "\n",
    "        def random_moving(data_numpy,\n",
    "                          angle_candidate=[-10., -5., 0., 5., 10.],\n",
    "                          scale_candidate=[0.9, 1.0, 1.1],\n",
    "                          transform_candidate=[-0.2, -0.1, 0.0, 0.1, 0.2],\n",
    "                          move_time_candidate=[1]):\n",
    "            # input: T,V,C\n",
    "            data_numpy = np.transpose(data_numpy, (2, 0, 1))\n",
    "            new_data_numpy = np.zeros(data_numpy.shape)\n",
    "            C, T, V = data_numpy.shape\n",
    "            move_time = random.choice(move_time_candidate)\n",
    "\n",
    "            node = np.arange(0, T, T * 1.0 / move_time).round().astype(int)\n",
    "            node = np.append(node, T)\n",
    "            num_node = len(node)\n",
    "\n",
    "            A = np.random.choice(angle_candidate, num_node)\n",
    "            S = np.random.choice(scale_candidate, num_node)\n",
    "            T_x = np.random.choice(transform_candidate, num_node)\n",
    "            T_y = np.random.choice(transform_candidate, num_node)\n",
    "\n",
    "            a = np.zeros(T)\n",
    "            s = np.zeros(T)\n",
    "            t_x = np.zeros(T)\n",
    "            t_y = np.zeros(T)\n",
    "\n",
    "            # linspace\n",
    "            for i in range(num_node - 1):\n",
    "                a[node[i]:node[i + 1]] = np.linspace(\n",
    "                    A[i], A[i + 1], node[i + 1] - node[i]) * np.pi / 180\n",
    "                s[node[i]:node[i + 1]] = np.linspace(S[i], S[i + 1],\n",
    "                                                     node[i + 1] - node[i])\n",
    "                t_x[node[i]:node[i + 1]] = np.linspace(T_x[i], T_x[i + 1],\n",
    "                                                       node[i + 1] - node[i])\n",
    "                t_y[node[i]:node[i + 1]] = np.linspace(T_y[i], T_y[i + 1],\n",
    "                                                       node[i + 1] - node[i])\n",
    "\n",
    "            theta = np.array([[np.cos(a) * s, -np.sin(a) * s],\n",
    "                              [np.sin(a) * s, np.cos(a) * s]])\n",
    "\n",
    "            # perform transformation\n",
    "            for i_frame in range(T):\n",
    "                xy = data_numpy[0:2, i_frame, :]\n",
    "                new_xy = np.dot(theta[:, :, i_frame], xy.reshape(2, -1))\n",
    "\n",
    "                new_xy[0] += t_x[i_frame]\n",
    "                new_xy[1] += t_y[i_frame]\n",
    "\n",
    "                new_data_numpy[0:2, i_frame, :] = new_xy.reshape(2, V)\n",
    "\n",
    "            new_data_numpy[2, :, :] = data_numpy[2, :, :]\n",
    "\n",
    "            return np.transpose(new_data_numpy, (1, 2, 0))\n",
    "\n",
    "        skeleton = np.array(skeleton)\n",
    "        skeletons = [skeleton]\n",
    "        if self.useTimeInterpolation:\n",
    "            skeletons.append(time_interpolate(skeleton))\n",
    "\n",
    "        if self.useNoise:\n",
    "            skeletons.append(noise(skeleton))\n",
    "\n",
    "        if self.useScaleAug:\n",
    "            skeletons.append(scale(skeleton))\n",
    "\n",
    "        if self.useTranslationAug:\n",
    "            skeletons.append(shift(skeleton))\n",
    "\n",
    "        if self.useSequenceFragments:\n",
    "            skeletons = [*skeletons, random_sequence_fragments(s)]\n",
    "\n",
    "        if self.useRandomMoving:\n",
    "            skeletons.append(random_moving(skeleton))\n",
    "\n",
    "        if self.useMirroring:\n",
    "            skeletons = [*skeletons, mirroring(s)]\n",
    "\n",
    "        return skeletons\n",
    "\n",
    "    def auto_padding(self, data_numpy, size, random_pad=False):\n",
    "        T, V, C = data_numpy.shape\n",
    "        if T < size:\n",
    "            begin = random.randint(0, size - T) if random_pad else 0\n",
    "            data_numpy_paded = np.zeros((size, V, C))\n",
    "            data_numpy_paded[begin:begin + T, :, :] = data_numpy\n",
    "            return data_numpy_paded\n",
    "        else:\n",
    "            return data_numpy\n",
    "\n",
    "    def upsample(self, skeleton, max_frames):\n",
    "        tensor = torch.unsqueeze(torch.unsqueeze(\n",
    "            torch.from_numpy(skeleton), dim=0), dim=0)\n",
    "\n",
    "        out = F.interpolate(\n",
    "            tensor, size=[max_frames, tensor.shape[-2], tensor.shape[-1]], mode='trilinear')\n",
    "        tensor = torch.squeeze(torch.squeeze(out, dim=0), dim=0)\n",
    "\n",
    "        return tensor\n",
    "\n",
    "    def sample_frames(self, data_num,sample_size):\n",
    "        # sample #window_size frames from whole video\n",
    "\n",
    "        each_num = (data_num - 1) / (sample_size - 1)\n",
    "        idx_list = [0, data_num - 1]\n",
    "        for i in range(sample_size):\n",
    "            index = round(each_num * i)\n",
    "            if index not in idx_list and index < data_num:\n",
    "                idx_list.append(index)\n",
    "        idx_list.sort()\n",
    "\n",
    "        while len(idx_list) < sample_size:\n",
    "            idx = random.randint(0, data_num - 1)\n",
    "            if idx not in idx_list:\n",
    "                idx_list.append(idx)\n",
    "        idx_list.sort()\n",
    "        return idx_list\n",
    "\n",
    "\n",
    "\n",
    "def load_data_sets(window_size=10, batch_size=32, workers=4, is_segmented=False):\n",
    "\n",
    "    train_ds = GraphDataset(\"./data/SHREC21\", \"training\", window_size=window_size,\n",
    "                            use_data_aug=False,\n",
    "                            normalize=False,\n",
    "                            scaleInvariance=False,\n",
    "                            translationInvariance=False,\n",
    "                            useRandomMoving=True,\n",
    "                            isPadding=False,\n",
    "                            useSequenceFragments=False,\n",
    "                            useMirroring=False,\n",
    "                            useTimeInterpolation=False,\n",
    "                            useNoise=True,\n",
    "                            useScaleAug=False,\n",
    "                            useTranslationAug=False,\n",
    "                            use_aug_by_sw=False,\n",
    "                            sample_classes=False,\n",
    "                            number_of_samples_per_class=23,\n",
    "                            is_segmented=is_segmented\n",
    "                            )\n",
    "    test_ds = GraphDataset(\"./data/SHREC21\", \"test\",\n",
    "                           window_size=window_size,\n",
    "                           use_data_aug=False,\n",
    "                           normalize=False,\n",
    "                           scaleInvariance=False,\n",
    "                           translationInvariance=False,\n",
    "                           isPadding=True,\n",
    "                           number_of_samples_per_class=14,\n",
    "                           use_aug_by_sw=False,\n",
    "                           sample_classes=False,\n",
    "                           is_segmented=is_segmented)\n",
    "    graph = Graph(layout=\"SHREC21\", strategy=\"distance\")\n",
    "    print(\"train data num: \", len(train_ds))\n",
    "    print(\"test data num: \", len(test_ds))\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size, shuffle=True,\n",
    "        num_workers=workers, pin_memory=False)\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        test_ds,\n",
    "        batch_size=batch_size, shuffle=False,\n",
    "        num_workers=workers, pin_memory=False)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_ds,\n",
    "        batch_size=1, shuffle=False,\n",
    "        num_workers=workers, pin_memory=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, torch.from_numpy(graph.A)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe5c41ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MaybeTensor=Union[Tensor, TensorPlaceholder]\n",
    "State = Tuple[\n",
    "    Tensor, \n",
    "    Tensor, \n",
    "    Tensor, \n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module, co.CoModule):\n",
    "    def __init__(self, dim_input: int = 128, dim_feedforward: int = 512):\n",
    "        super().__init__()\n",
    "        self.call_mode = CallMode.FORWARD_STEPS\n",
    "        self.out=nn.Sequential(\n",
    "        nn.Linear(dim_input, dim_feedforward,dtype=torch.float).cuda(),\n",
    "        nn.Mish(),\n",
    "        nn.Linear(dim_feedforward, dim_input,dtype=torch.float).cuda(),\n",
    "    )\n",
    "    def clean_state(self):\n",
    "        pass\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.out(x) \n",
    "class Residual(nn.Module, co.CoModule):\n",
    "    def __init__(self, sublayer: nn.Module, dimension: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.call_mode = CallMode.FORWARD_STEPS\n",
    "        self.sublayer = sublayer\n",
    "        self.norm = nn.LayerNorm(dimension,dtype=torch.float).cuda()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def clean_state(self):\n",
    "        self.sublayer.clean_state()\n",
    "    def forward_steps(self, x: Tensor) -> Tensor:\n",
    "        return self.forward(x)\n",
    "    def forward(self, *tensors: Tensor) -> Tensor:\n",
    "        # Assume that the \"query\" tensor is given first, so we can compute the\n",
    "        # residual.  This matches the signature of 'MultiHeadAttention'.\n",
    "        x=self.dropout(self.sublayer(*tensors))\n",
    "        # print(x.shape)\n",
    "        # print(tensors[0].shape)\n",
    "        x=tensors[0] + x\n",
    "        x=self.norm(x)\n",
    "        return x\n",
    "\n",
    "def _scaled_dot_product_attention_default_state(\n",
    "    batch_size: int,\n",
    "    sequence_len: int,\n",
    "    num_nodes : int,\n",
    "    embed_dim_k: int,\n",
    "    embed_dim_v: int,\n",
    "    query_index=-1,\n",
    "    init_fn=torch.zeros,\n",
    "    dtype=None,\n",
    "    device=None,\n",
    "):\n",
    "    init_fn = partial(init_fn, dtype=dtype, device=device)\n",
    "    B = batch_size\n",
    "    V=num_nodes\n",
    "    N = sequence_len\n",
    "    Nq = sequence_len\n",
    "    Q_mem = init_fn((B, V, Nq, embed_dim_k)).float()\n",
    "    K_T_mem = init_fn((B, V, embed_dim_k, N)).float()\n",
    "    V_mem = init_fn((B, V, N, embed_dim_v)).float()\n",
    "    return (Q_mem, K_T_mem, V_mem)\n",
    "\n",
    "def _clone_state(state):\n",
    "    return [s.clone() for s in state]\n",
    "\n",
    "def _scaled_dot_product_attention_step(\n",
    "    prev_state: State,\n",
    "    q_step: Tensor,  # step input (B, E)\n",
    "    k_step: Tensor,  # step input (B, E)\n",
    "    v_step: Tensor,  # step input (B, E)\n",
    "    T,\n",
    "    dropout_p: float = 0.0,\n",
    ") -> Tuple[Tensor, State]:\n",
    "    \"\"\"\n",
    "    Computes the Continual Singe-output Scaled Dot-Product Attention on query, key and value tensors.\n",
    "    Returns attended values and updated states.\n",
    "\n",
    "    Args:\n",
    "        q_step, k_step, v_step: query, key and value tensors for a step. See Shape section for shape details.\n",
    "        attn_mask: optional tensor containing mask values to be added to calculated\n",
    "            attention. May be 2D or 3D; see Shape section for details.\n",
    "        dropout_p: dropout probability. If greater than 0.0, dropout is applied.\n",
    "\n",
    "    Shape:\n",
    "        - q_step: :math:`(B, V, E)` where B is batch size, V is the number of vertices and E is embedding dimension.\n",
    "        - k_step: :math:`(B, V, E)` where B is batch size, V is the number of vertices and E is embedding dimension.\n",
    "        - v_step: :math:`(B, V, E)` where B is batch size, V is the number of vertices and E is embedding dimension.\n",
    "\n",
    "        - Output: attention values have shape :math:`(B, Nt, E)`; new state\n",
    "    \"\"\"\n",
    "    # if attn_mask is not None:\n",
    "    #     logger.warning(\"attn_mask is not supported yet and will be skipped\")\n",
    "    # if dropout_p != 0.0:\n",
    "    #     logger.warning(\"dropout_p is not supported yet and will be skipped\")\n",
    "    \n",
    "    (\n",
    "        Q_mem,  # (B, V, Nq, E)\n",
    "        K_T_mem,  # (B, V, E, Ns)\n",
    "        V_mem,  # (B, V, Ns, E)\n",
    "    ) = prev_state\n",
    "    # print(Q_mem)\n",
    "    B, V, E = q_step.shape\n",
    "    q_step = q_step / math.sqrt(E)\n",
    "    q_sel = (Q_mem[:B,:, 0] if Q_mem.shape[2] > 0 else q_step).unsqueeze(2).cuda()\n",
    "    # Update states\n",
    "    # Note: We're allowing the K and V mem to have one more entry than\n",
    "    # strictly necessary to simplify computatations.\n",
    "    K_T_new = torch.roll(K_T_mem, shifts=-1, dims=(3,))\n",
    "    K_T_new[:B, :, :, -1] = k_step\n",
    "    V_new = torch.roll(V_mem, shifts=-1, dims=(2,))\n",
    "    V_new[:B, :, -1] = v_step\n",
    "    \n",
    "    attn = torch.bmm(q_sel.reshape(-1,1,E), K_T_new[:q_sel.shape[0]].reshape(-1,E,T).cuda())\n",
    "    K_T_new=K_T_new.detach().cpu()\n",
    "    attn_sm = F.softmax(attn, dim=-1)\n",
    "    \n",
    "    if dropout_p > 0.0:\n",
    "        attn_sm = F.dropout(attn_sm, p=dropout_p)\n",
    "    \n",
    "    # (B, V, Nt, Ns) x (B, V, Ns, E) -> (B, V, Nt, E)\n",
    "    output = torch.bmm(attn_sm, V_new[:B].reshape(-1,T,E).cuda()).reshape(B,V,-1,E)\n",
    "    \n",
    "    if Q_mem.shape[2] > 0:\n",
    "        Q_new = torch.roll(Q_mem, shifts=-1, dims=(2,))\n",
    "        Q_new[:B, :, -1] = q_step.detach().cpu()\n",
    "    else:\n",
    "        Q_new = Q_mem\n",
    "    new_states = (Q_new, K_T_new, V_new.detach().cpu())\n",
    "    \n",
    "    return output, new_states\n",
    "\n",
    "\n",
    "class AttentionHead(nn.Module, co.CoModule):\n",
    "    def __init__(self, is_continual : bool, dim_in: int, dim_v: int, dim_k: int, kernel_size: int = 1 , stride :int =1, dropout :int=.1):\n",
    "        super().__init__()\n",
    "        self.call_mode = CallMode.FORWARD_STEPS if is_continual else CallMode.FORWARD\n",
    "        self.embed_dim_second=False\n",
    "        self.batch_first=True\n",
    "        self.d_k=dim_k\n",
    "        self.d_v=dim_v\n",
    "        self.dropout=dropout\n",
    "        self.q_conv=co.Conv2d(\n",
    "                dim_in,\n",
    "                dim_k,\n",
    "                kernel_size=(kernel_size, 1),\n",
    "                padding=(int((kernel_size - 1) / 2), 0),\n",
    "                stride=(stride, 1),dtype=torch.float).cuda()\n",
    "        self.k_conv=co.Conv2d(\n",
    "                dim_in,\n",
    "                dim_k,\n",
    "                kernel_size=(kernel_size, 1),\n",
    "                padding=(int((kernel_size - 1) / 2), 0),\n",
    "                stride=(stride, 1),dtype=torch.float).cuda()\n",
    "        self.v_conv=co.Conv2d(\n",
    "                dim_in,\n",
    "                dim_v,\n",
    "                kernel_size=(kernel_size, 1),\n",
    "                padding=(int((kernel_size - 1) / 2), 0),\n",
    "                stride=(stride, 1),dtype=torch.float).cuda()\n",
    "\n",
    "    def get_state(self) -> Optional[State]:\n",
    "        \"\"\"Get model state\n",
    "\n",
    "        Returns:\n",
    "            Optional[State]: A State tuple if the model has been initialised and otherwise None.\n",
    "        \"\"\"\n",
    "        if (\n",
    "            getattr(self, \"Q_mem\", None) is not None\n",
    "            and getattr(self, \"K_T_mem\", None) is not None\n",
    "            and getattr(self, \"V_mem\", None) is not None\n",
    "            and getattr(self, \"stride_index\", None) is not None\n",
    "        ):\n",
    "            return (\n",
    "                self.Q_mem,\n",
    "                self.K_T_mem,\n",
    "                self.V_mem,\n",
    "                self.stride_index,\n",
    "            )\n",
    "\n",
    "    def set_state(self, state: State):\n",
    "        \"\"\"Set model state\n",
    "\n",
    "        Args:\n",
    "            state (State): State tuple to set as new internal internal state\n",
    "        \"\"\"\n",
    "        (\n",
    "            self.Q_mem,\n",
    "            self.K_T_mem,\n",
    "            self.V_mem,\n",
    "            self.stride_index,\n",
    "        ) = state\n",
    "\n",
    "    def clean_state(self):\n",
    "        \"\"\"Clean model state\"\"\"\n",
    "        if hasattr(self, \"Q_mem\"):\n",
    "            del self.Q_mem\n",
    "        \n",
    "        if hasattr(self, \"K_T_mem\"):\n",
    "            del self.K_T_mem\n",
    "        if hasattr(self, \"V_mem\"):\n",
    "            del self.V_mem\n",
    "        if hasattr(self, \"stride_index\"):\n",
    "            del self.stride_index\n",
    "\n",
    "    def _forward_step(\n",
    "        self,\n",
    "        query: Tensor,\n",
    "        key: Tensor,\n",
    "        value: Tensor,\n",
    "        T: int,\n",
    "        prev_state: State = None,\n",
    "    ) -> Tuple[MaybeTensor, State]:\n",
    "        \"\"\"Forward computation for a single step with state initialisation\n",
    "\n",
    "        Args:\n",
    "            query, key, value: step inputs of shape `(B, E)` where B is the batch size and E is the embedding dimension.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[MaybeTensor, State]: Step output and new state.\n",
    "        \"\"\"\n",
    "        B, V, E = query.shape\n",
    "        if prev_state is None:\n",
    "            prev_state = (\n",
    "                *_scaled_dot_product_attention_default_state(B, T, V, self.d_k, self.d_v),\n",
    "                -T,\n",
    "            )\n",
    "\n",
    "        o, new_state = _scaled_dot_product_attention_step(\n",
    "            prev_state[:-1],\n",
    "            query,\n",
    "            key,\n",
    "            value,\n",
    "            T,\n",
    "            self.dropout,\n",
    "        )\n",
    "        stride_index = prev_state[-1]\n",
    "        if stride_index < 0:\n",
    "            stride_index += 1\n",
    "\n",
    "        new_state = (*new_state, stride_index)\n",
    "    \n",
    "        return (\n",
    "             o,\n",
    "            new_state,\n",
    "        )\n",
    "\n",
    "    def forward_step(\n",
    "        self,\n",
    "        T: int,\n",
    "        query: Tensor,\n",
    "        key: Tensor = None,\n",
    "        value: Tensor = None,\n",
    "        update_state=True,\n",
    "    ) -> MaybeTensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query, key, value: step_inputs for mapping a query and a set of key-value pairs to an output.\n",
    "                See \"Attention Is All You Need\" for more details.\n",
    "\n",
    "        Shapes for inputs:\n",
    "            - query: :math:`(N, E)` where L is the target sequence length, N is the batch size, E is\n",
    "              the embedding dimension.\n",
    "            - key: :math:`(N, E)`, where S is the source sequence length, N is the batch size, E is\n",
    "              the embedding dimension.\n",
    "            - value: :math:`(N, E)` where S is the source sequence length, N is the batch size, E is\n",
    "              the embedding dimension.\n",
    "\n",
    "        Shapes for outputs:\n",
    "            - attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size,\n",
    "              E is the embedding dimension. :math:`(N, L, E)` if ``batch_first`` is ``True``.\n",
    "        \"\"\"\n",
    "        if key is None:\n",
    "            key = query\n",
    "        if value is None:\n",
    "            value = query\n",
    "\n",
    "        tmp_state = self.get_state()\n",
    "\n",
    "        if not update_state and tmp_state:\n",
    "            backup_state = _clone_state(tmp_state)\n",
    "\n",
    "        o, tmp_state = self._forward_step(query, key, value, T, tmp_state)\n",
    "        if self.batch_first and not isinstance(o, TensorPlaceholder):\n",
    "            o = o.transpose(1, 0)\n",
    "\n",
    "        if update_state:\n",
    "            self.set_state(tmp_state)\n",
    "        elif tmp_state is not None:\n",
    "            self.set_state(backup_state)\n",
    "\n",
    "        return o\n",
    "\n",
    "    def forward_steps(\n",
    "        self,\n",
    "        x : Tensor,\n",
    "        update_state=True,\n",
    "    ) -> MaybeTensor:\n",
    "        \"\"\"Forward computation for multiple steps with state initialisation\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): input.\n",
    "            update_state (bool): Whether internal state should be updated during this operation.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Stepwise layer outputs\n",
    "        \"\"\"\n",
    "        _, T, _, _ = x.shape\n",
    "        query, key, value= self.projection(x)\n",
    "        # if key is None:\n",
    "        #     key = query\n",
    "        # if value is None:\n",
    "        #     value = query\n",
    "\n",
    "        if self.embed_dim_second:\n",
    "            # N E V T -> N T V E\n",
    "            query = query.permute(0, 3, 2, 1)\n",
    "            key = key.permute(0, 3, 2, 1)\n",
    "            value = value.permute(0, 3, 2, 1)\n",
    "\n",
    "        if self.batch_first:\n",
    "            # N T V E -> T N V E\n",
    "            query, key, value = [x.transpose(1, 0) for x in (query, key, value)]\n",
    "\n",
    "        tmp_state = self.get_state()\n",
    "\n",
    "        if not update_state and tmp_state:\n",
    "            backup_state = _clone_state(tmp_state)\n",
    "        T = query.shape[0]\n",
    "        assert T == key.shape[0]\n",
    "        assert T == value.shape[0]\n",
    "        outs = []\n",
    "\n",
    "        for t in range(T):\n",
    "            # print(t)\n",
    "            o, tmp_state = self._forward_step(query[t], key[t], value[t], T, tmp_state)\n",
    "            if isinstance(o, Tensor):\n",
    "                if self.batch_first:\n",
    "                    o = o.transpose(0, 1)\n",
    "                outs.append(o)\n",
    "        # print(\"here\",T,len(outs),outs[0].shape)\n",
    "\n",
    "        if update_state:\n",
    "            self.set_state(tmp_state)\n",
    "        elif backup_state is not None:\n",
    "            self.set_state(backup_state)\n",
    "\n",
    "        if len(outs) == 0:\n",
    "            return o\n",
    "\n",
    "        o = torch.stack(outs, dim=2 ).squeeze(3).permute(1,2,0,3)\n",
    "\n",
    "        return o\n",
    "\n",
    "    def attention(self,Q,K,V):\n",
    "      sqrt_dk=torch.sqrt(torch.tensor(self.d_k))\n",
    "      attention_weights=F.softmax((Q @ K.transpose(-2,-1))/sqrt_dk)\n",
    "      attention_vectors=attention_weights @ V\n",
    "      return attention_vectors\n",
    "\n",
    "    def projection(self,x: Tensor):\n",
    "        \n",
    "        x=x.permute(0,3,2,1)\n",
    "        Q=self.q_conv(x).permute(0,3,2,1)\n",
    "        K=self.k_conv(x).permute(0,3,2,1)\n",
    "        V=self.v_conv(x).permute(0,3,2,1)\n",
    "        return Q, K, V\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "        seq_length = x.size(1)\n",
    "        graph_size=x.size(2)\n",
    "        \n",
    "        x=x.permute(0,3,2,1)\n",
    "        # x=x.transpose(1,2)\n",
    "        #Q, K, V=torch.split(self.qkv_conv(x), [self.d_k , self.d_k, self.d_v],\n",
    "        #                            dim=1)\n",
    "        Q=self.q_conv(x).permute(0,3,2,1)\n",
    "        K=self.k_conv(x).permute(0,3,2,1)\n",
    "        V=self.v_conv(x).permute(0,3,2,1)\n",
    "\n",
    "\n",
    "\n",
    "        x=self.attention(Q,K,V).transpose(1,2).contiguous().view(batch_size,seq_length,graph_size, self.d_k)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class MultiHeadAttention(co.CoModule,nn.Module):\n",
    "    def __init__(self, is_continual: bool, num_heads: int, dim_in: int,dim_k,dim_q,dim_v,dropout):\n",
    "        super().__init__()\n",
    "        self.call_mode = CallMode.FORWARD_STEPS if is_continual else CallMode.FORWARD\n",
    "        self.heads = nn.ModuleList(\n",
    "            [AttentionHead(is_continual,dim_in, dim_v, dim_k,dropout=dropout) for _ in range(num_heads)]\n",
    "        )\n",
    "        self.linear = nn.Linear(num_heads * dim_k, dim_in,dtype=torch.float).cuda()\n",
    "    def clean_state(self):\n",
    "        for h in self.heads:\n",
    "            h.clean_state()\n",
    "    \n",
    "    def forward_steps(self, x: Tensor, pad_end=False, update_state=True) -> Tensor:\n",
    "        out=self.linear(\n",
    "            torch.cat([h.forward_steps(x) for h in self.heads], dim=-1)\n",
    "        ).cuda()\n",
    "        \n",
    "        return out\n",
    "    def forward(self, x) -> Tensor:\n",
    "        return self.linear(\n",
    "            torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        )\n",
    "\n",
    "class TransformerGraphEncoderLayer(nn.Module, co.CoModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        is_continual:bool=False,\n",
    "        dim_model: int = 128,\n",
    "        num_heads: int = 8,\n",
    "        dim_feedforward: int = 512,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.call_mode = CallMode.FORWARD_STEPS if is_continual else CallMode.FORWARD\n",
    "        dim_v=dim_q = dim_k = max(dim_model // num_heads, 1)\n",
    "        self.attention = Residual(\n",
    "            MultiHeadAttention(is_continual,num_heads, dim_model,32,32,32,dropout),\n",
    "            dimension=dim_model,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.feed_forward = Residual(\n",
    "            FeedForward(dim_model, dim_feedforward),\n",
    "            dimension=dim_model,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(dim_model,dtype=torch.float).cuda()\n",
    "    def clean_state(self):\n",
    "        self.attention.clean_state()\n",
    "    def forward(self, src: Tensor) -> Tensor:\n",
    "        # print(\"before\",torch.cuda.mem_get_info(torch.device('cuda:0')))\n",
    "        src = self.attention(self.norm(src))\n",
    "        # print(\"after\",torch.cuda.mem_get_info(torch.device('cuda:0')))\n",
    "        return self.feed_forward(src)\n",
    "\n",
    "class PositionalEncoder(nn.Module, co.CoModule):\n",
    "    def __init__(self, d_model, max_seq_len = 200):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # create constant 'pe' matrix with values dependant on z\n",
    "        # pos and i\n",
    "        pe = torch.zeros(max_seq_len,20 , d_model)\n",
    "        for pos in range(max_seq_len):\n",
    "          for node_id in range(0,20) :\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, node_id, i] = \\\n",
    "                math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                pe[pos, node_id, i + 1] = \\\n",
    "                math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
    "                \n",
    "        pe = pe.unsqueeze(0)\n",
    "        #self.learnable_pe=nn.Linear(d_model, d_model,dtype=torch.float)\n",
    "        self.norm=nn.LayerNorm(d_model,dtype=torch.float).cuda()\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # make embeddings relatively larger\n",
    "        # x = x * math.sqrt(self.d_model)\n",
    "        #add constant to embedding\n",
    "        seq_len = x.size(1)\n",
    "        \n",
    "        x = self.norm(x + Variable(self.pe[:,:seq_len,:,:], \\\n",
    "        requires_grad=False).cuda())\n",
    "        \n",
    "        return x\n",
    "\n",
    "class TransformerGraphEncoder(nn.Module, co.CoModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        is_continual: bool=False,\n",
    "        num_layers: int = 6,\n",
    "        dim_model: int = 128,\n",
    "        num_heads: int = 8,\n",
    "        dim_feedforward: int = 512,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.call_mode = CallMode.FORWARD_STEPS if is_continual else CallMode.FORWARD\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "            TransformerGraphEncoderLayer(is_continual,dim_model, num_heads, dim_feedforward, dropout)      \n",
    "            for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.positional_encoder=PositionalEncoder(dim_model)\n",
    "    def clean_state(self):\n",
    "        for layer in self.layers:\n",
    "            layer.clean_state()\n",
    "    def forward_steps(self, x: Tensor, pad_end=False, update_state=True) -> Tensor:\n",
    "        return self.forward(x)\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x += self.positional_encoder(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        # if self.call_mode==CallMode.FORWARD_STEPS:\n",
    "        #     self.clean_state()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9373413",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def conv_init(conv):\n",
    "    nn.init.kaiming_normal_(conv.weight, mode='fan_out')\n",
    "    nn.init.constant_(conv.bias, 0)\n",
    "\n",
    "\n",
    "class unit_gcn(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 A,\n",
    "                 use_local_bn=False,\n",
    "                 kernel_size=1,\n",
    "                 stride=1,\n",
    "                 mask_learning=False):\n",
    "        super(unit_gcn, self).__init__()\n",
    "\n",
    "        # ==========================================\n",
    "        # number of nodes\n",
    "        self.V = 20\n",
    "\n",
    "        # the adjacency matrixes of the graph\n",
    "        # self.A = Variable(\n",
    "        #     A.clone(), requires_grad=False).view(-1, self.V, self.V)\n",
    "\n",
    "        # number of input channels\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        # number of output channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        # if true, use mask matrix to reweight the adjacency matrix\n",
    "        self.mask_learning = mask_learning\n",
    "\n",
    "        # number of adjacency matrix (number of partitions)\n",
    "        self.num_A = A.size(0)\n",
    "\n",
    "        # if true, each node have specific parameters of batch normalizaion layer.\n",
    "        # if false, all nodes share parameters.\n",
    "        self.use_local_bn = use_local_bn\n",
    "        # ==========================================\n",
    "\n",
    "        self.conv_list = nn.ModuleList([\n",
    "            nn.Conv2d(\n",
    "                self.in_channels,\n",
    "                self.out_channels,\n",
    "                kernel_size=(kernel_size, 1),\n",
    "                padding=(int((kernel_size - 1) / 2), 0),\n",
    "                stride=(stride, 1), dtype=torch.float).cuda() for i in range(self.num_A)\n",
    "        ])\n",
    "\n",
    "        if mask_learning:\n",
    "            self.mask = nn.Parameter(torch.ones(A.size())).cuda()\n",
    "        if use_local_bn:\n",
    "            self.bn = nn.BatchNorm1d(self.out_channels * self.V).cuda()\n",
    "        else:\n",
    "            self.bn = nn.BatchNorm2d(self.out_channels, dtype=torch.float).cuda()\n",
    "\n",
    "        self.act = nn.Mish()\n",
    "\n",
    "        # initialize\n",
    "        for conv in self.conv_list:\n",
    "            conv_init(conv)\n",
    "\n",
    "    def forward(self, x, A):\n",
    "        \n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "\n",
    "        N, C, T, V = x.size()\n",
    "        A = A.cuda(x.get_device())\n",
    "\n",
    "        # reweight adjacency matrix\n",
    "        if self.mask_learning:\n",
    "            A = A*self.mask.cuda()\n",
    "        # graph convolution\n",
    "        for i, a in enumerate(A):\n",
    "\n",
    "            xa = x.reshape(-1, V).mm(a).reshape(N, C, T, V)\n",
    "\n",
    "            if i == 0:\n",
    "                y = self.conv_list[i](xa)\n",
    "            else:\n",
    "                y = y+self.conv_list[i](xa)\n",
    "\n",
    "        # batch normalization\n",
    "        if self.use_local_bn:\n",
    "            y = y.permute(0, 1, 3, 2).contiguous().view(\n",
    "                N, self.out_channels * V, T)\n",
    "            y = self.bn(y)\n",
    "            y = y.view(N, self.out_channels, V, T).permute(0, 1, 3, 2)\n",
    "        else:\n",
    "            y = self.bn(y.clone())\n",
    "\n",
    "        # nonliner\n",
    "        y = self.act(y.clone())\n",
    "\n",
    "        y = y.clone().permute(0, 2, 3, 1)\n",
    "        return y\n",
    "\n",
    "\n",
    "class SGCN(nn.Module):\n",
    "    def __init__(self, features_in, features_out, A) -> None:\n",
    "        super().__init__()\n",
    "        default_backbone = [(features_in, 64, 1), (64, 64, 1), (64, 64, 1), (64, 64, 1), (64, features_out, 2), (features_out, features_out, 1),(features_out, features_out, 1)]\n",
    "        # , (128, 256, 2), (256, 256, 1), (256, 256, 1) , (256, 512, 2), (512, 512, 1), (512, 512, 1)\n",
    "        # default_backbone = [(3,128,1)]\n",
    "        self.conv_layers = nn.ModuleList([\n",
    "            # unit_agcn(dim_in, dim_out, A)\n",
    "            unit_gcn(dim_in, dim_out, A, mask_learning=True)\n",
    "            for dim_in, dim_out, kernel_size in default_backbone\n",
    "        ])\n",
    "\n",
    "    def forward(self, x: Tensor, adjacency_matrix: Tensor) -> torch.Tensor:\n",
    "        for l in self.conv_layers:\n",
    "            x = l(x, adjacency_matrix)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad1442b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# model definition\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "class STrGCN(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, adjacency_matrix,optimizer_params, labels, num_classes : int=18, d_model: int=512, n_heads: int=8,\n",
    "                 nEncoderlayers: int=6, dropout: float = 0.1):\n",
    "        super(STrGCN, self).__init__()\n",
    "        # not the best model...\n",
    "        self.labels=labels\n",
    "        features_in=3       \n",
    "        self.cnf_matrix= torch.zeros(num_classes, num_classes).cuda()\n",
    "        self.Learning_Rate, self.betas, self.epsilon, self.weight_decay=optimizer_params\n",
    "        self.num_classes=num_classes\n",
    "        self.adjacency_matrix=adjacency_matrix.float()\n",
    "        self.is_continual=True\n",
    "        self.train_acc = torchmetrics.Accuracy()\n",
    "        self.valid_acc = torchmetrics.Accuracy()\n",
    "        self.test_acc = torchmetrics.Accuracy()\n",
    "        self.val_f1_score=torchmetrics.F1Score(num_classes)\n",
    "        self.train_f1_score=torchmetrics.F1Score(num_classes)\n",
    "        self.test_f1_score=torchmetrics.F1Score(num_classes)\n",
    "        self.val_jaccard=torchmetrics.JaccardIndex(num_classes)\n",
    "        self.train_jaccard=torchmetrics.JaccardIndex(num_classes)\n",
    "        self.test_jaccard=torchmetrics.JaccardIndex(num_classes)\n",
    "        self.confusion_matrix=torchmetrics.ConfusionMatrix(num_classes)\n",
    "        self.gcn=SGCN(features_in,d_model,self.adjacency_matrix)\n",
    "\n",
    "        self.encoder=TransformerGraphEncoder(is_continual=self.is_continual,dropout=dropout,num_heads=n_heads,dim_model=d_model, num_layers=nEncoderlayers)\n",
    "\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model,dtype=torch.float).cuda(),\n",
    "            nn.Mish(),\n",
    "            # nn.Dropout(dropout),\n",
    "            nn.LayerNorm(d_model,dtype=torch.float).cuda(),\n",
    "            nn.Linear(d_model,num_classes,dtype=torch.float).cuda()\n",
    "          )\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.init_parameters()\n",
    "    def init_parameters(self):\n",
    "        for name,p in self.named_parameters() :\n",
    "          if p.dim() > 1:\n",
    "              nn.init.xavier_uniform_(p)\n",
    "    def get_fp_rate(self,score,labels):\n",
    "        \n",
    "\n",
    "\n",
    "        cnf_matrix = self.confusion_matrix(score.detach().cpu(), labels.detach().cpu())\n",
    "        FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix)\n",
    "        FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
    "        TP = np.diag(cnf_matrix)\n",
    "        TN = cnf_matrix.sum() - (FP + FN + TP)\n",
    "\n",
    "        FP = FP.type(torch.float)\n",
    "        TN = TN.type(torch.float)\n",
    "\n",
    "        # # Sensitivity, hit rate, recall, or true positive rate\n",
    "        # TPR = TP/(TP+FN)\n",
    "        # # Specificity or true negative rate\n",
    "        # TNR = TN/(TN+FP)\n",
    "        # # Precision or positive predictive value\n",
    "        # PPV = TP/(TP+FP)\n",
    "        # # Negative predictive value\n",
    "        # NPV = TN/(TN+FN)\n",
    "        # # Fall out or false positive rate\n",
    "        FPR = FP/(FP+TN)\n",
    "        # # False negative rate\n",
    "        # FNR = FN/(TP+FN)\n",
    "        # # False discovery rate\n",
    "        # FDR = FP/(TP+FP)\n",
    "        # # Overall accuracy\n",
    "        # ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "        return torch.sum(torch.nan_to_num(FPR),dim=-1) \n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        x=x.type(torch.float).cuda() \n",
    "        \n",
    "        # print(x.shape)\n",
    "        #spatial features from SGCN\n",
    "        x=self.gcn(x,self.adjacency_matrix)\n",
    "        \n",
    "        # print(x.shape)\n",
    "        # print(x.shape)\n",
    "        # temporal features from TGE\n",
    "        x=self.encoder(x)\n",
    "        \n",
    "        \n",
    "        # print(x.shape)\n",
    "\n",
    "        # Global average pooling\n",
    "        N,T,V,C=x.shape\n",
    "        x=x.permute(0,3,1,2)\n",
    "        # V pooling\n",
    "        x = F.avg_pool2d(x, kernel_size=(1, V)).view(N,C,T)\n",
    "        \n",
    "        # T pooling\n",
    "        x = F.avg_pool1d(x, kernel_size=T).view(N,C)\n",
    "        \n",
    "        # print(x)\n",
    "        # Classifier\n",
    "        x=self.out(x)\n",
    "        # print(torch.equal(x[0],x[1]))\n",
    "        \n",
    "        return x\n",
    "    def plot_confusion_matrix(self,filename,eps=1e-5) :\n",
    "        import seaborn as sn\n",
    "        confusion_matrix_sum_vec= torch.sum(self.cnf_matrix,dim=1) +eps\n",
    "        \n",
    "        confusion_matrix_percentage=(self.cnf_matrix /  confusion_matrix_sum_vec.view(-1,1) )\n",
    "\n",
    "        plt.figure(figsize = (18,16))\n",
    "        sn.heatmap(confusion_matrix_percentage.cpu().numpy(), annot=True,cmap=\"coolwarm\", xticklabels=self.labels,yticklabels=self.labels)\n",
    "        plt.savefig(filename,format=\"eps\")\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        # REQUIRED\n",
    "        x = batch[0].float()\n",
    "        y = batch[1]\n",
    "        y = y.type(torch.LongTensor)\n",
    "        y = y.cuda()\n",
    "        y = Variable(y, requires_grad=False)\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        # print(loss)\n",
    "        # input()\n",
    "        #l1 regularization\n",
    "        l1_lambda = 1e-4\n",
    "        l1_norm = sum( p.abs().sum()  for p in self.parameters())\n",
    "\n",
    "        loss_with_l1 = loss + l1_lambda * l1_norm\n",
    "\n",
    "        self.train_acc(y_hat, y)\n",
    "        self.train_f1_score(y_hat, y)\n",
    "        \n",
    "        self.log('train_loss', loss,on_epoch=True,on_step=True)\n",
    "        self.log('train_acc', self.train_acc.compute(), prog_bar=True, on_step=True, on_epoch=True)\n",
    "\n",
    "        # self.log('train_F1_score', self.train_f1_score.compute(), prog_bar=True, on_step=True, on_epoch=True)\n",
    "        # self.log('train_Jaccard', self.train_jaccard(y_hat, y), prog_bar=True, on_step=True, on_epoch=True)\n",
    "        # self.log('train_FP_rate', self.get_fp_rate(torch.argmax(torch.nn.functional.softmax(y_hat, dim=-1), dim=-1), y), prog_bar=True, on_step=True, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        # OPTIONAL\n",
    "        \n",
    "        x = batch[0].float()\n",
    "        y = batch[1]\n",
    "        y = y.type(torch.LongTensor)\n",
    "        y = y.cuda()\n",
    "        targets = Variable(y, requires_grad=False)\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, targets)\n",
    "        # print(loss)\n",
    "        # input()\n",
    "        self.valid_acc(y_hat, y)\n",
    "        self.val_f1_score(y_hat, y)\n",
    "        \n",
    "        self.log('val_loss', loss, prog_bar=True,on_epoch=True,on_step=True)\n",
    "        self.log('val_accuracy', self.valid_acc.compute(), prog_bar=True, on_step=True, on_epoch=True)\n",
    "\n",
    "        # self.log('val_F1_score', self.val_f1_score.compute(), prog_bar=True, on_step=True, on_epoch=True)\n",
    "        # self.log('val_Jaccard', self.val_jaccard(y_hat, y), prog_bar=True, on_step=True, on_epoch=True)\n",
    "        # self.log('val_FP_rate', self.get_fp_rate(torch.argmax(torch.nn.functional.softmax(y_hat, dim=-1), dim=-1), y), prog_bar=True, on_step=True, on_epoch=True)\n",
    "\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        #for name,p in self.named_parameters() :\n",
    "        #    print(p.shape)\n",
    "        \n",
    "        self.train_acc.reset()\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        self.valid_acc.reset()\n",
    "\n",
    "    def test_step(self, batch, batch_nb):\n",
    "        # global confusion_matrix\n",
    "        # OPTIONAL\n",
    "        x = batch[0].float()\n",
    "        y = batch[1]\n",
    "        y = y.type(torch.LongTensor)\n",
    "        y = y.cuda()\n",
    "        targets = Variable(y, requires_grad=False)\n",
    "        y_hat = self(x)\n",
    "        _, preds = torch.max(y_hat, 1)\n",
    "        self.test_acc(y_hat, targets)\n",
    "        self.test_f1_score(y_hat, y)\n",
    "        \n",
    "        loss = F.cross_entropy(y_hat, targets)        \n",
    "        self.log('test_loss', loss, prog_bar=True)\n",
    "        self.log('test_accuracy', self.test_acc.compute(), prog_bar=True)\n",
    "\n",
    "        # self.log('test_F1_score', self.val_f1_score.compute(), prog_bar=True)\n",
    "        # self.log('test_Jaccard', self.test_jaccard(y_hat, y), prog_bar=True, on_step=True, on_epoch=True)\n",
    "        # self.log('test_FP_rate', self.get_fp_rate(torch.argmax(torch.nn.functional.softmax(y_hat, dim=-1), dim=-1), y), prog_bar=True, on_step=True, on_epoch=True)\n",
    "\n",
    "        \n",
    "        self.cnf_matrix+=self.confusion_matrix(preds,targets)\n",
    "\n",
    "    def on_test_end(self):\n",
    "        time_now=datetime.today().strftime('%Y-%m-%d_%H_%M_%S')\n",
    "        self.plot_confusion_matrix(f\"./Confusion_matrices/Confusion_matrix_{time_now}.eps\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # REQUIRED\n",
    "        # can return multiple optimizers and learning_rate schedulers\n",
    "        # (LBFGS it is automatically supported, no need for closure function)\n",
    "        \n",
    "\n",
    "        opt = torch.optim.RAdam(filter(lambda p: p.requires_grad, self.parameters()), lr=self.Learning_Rate, weight_decay=self.weight_decay)\n",
    "        reduce_lr_on_plateau = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            opt,\n",
    "            mode='min',\n",
    "            factor=.5,\n",
    "            patience=2,\n",
    "            min_lr=1e-4,\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "        return  {\"optimizer\": opt, \"lr_scheduler\": reduce_lr_on_plateau, \"monitor\": \"val_loss\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b50c14d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 108/108 [00:22<00:00,  4.78it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 72/72 [00:12<00:00,  5.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data num:  108\n",
      "test data num:  72\n",
      "\n",
      " loading model.............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wrabah-externe\\Anaconda3\\envs\\alt_env\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:36: UserWarning: Metric `AveragePrecision` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.metrics import confusion_matrix\n",
    "labels = [\n",
    "    \"\",\n",
    "    \"RIGHT\",\n",
    "    \"KNOB\",\n",
    "    \"CROSS\",\n",
    "    \"THREE\",\n",
    "    \"V\",\n",
    "    \"ONE\",\n",
    "    \"FOUR\",\n",
    "    \"GRAB\",\n",
    "    \"DENY\",\n",
    "    \"MENU\",\n",
    "    \"CIRCLE\",\n",
    "    \"TAP\",\n",
    "    \"PINCH\",\n",
    "    \"LEFT\",\n",
    "    \"TWO\",\n",
    "    \"OK\",\n",
    "    \"EXPAND\",\n",
    "]\n",
    "DATASETS_PATH = \"datasets/\"\n",
    "DS_NAME = \"shrec21\"\n",
    "DS_PATH = DATASETS_PATH + \"shrec21/\"\n",
    "batch_size = 32\n",
    "workers = 4\n",
    "lr = 1e-4\n",
    "num_classes = 18\n",
    "window_size=10\n",
    "input_shape = (window_size,20,3)\n",
    "device = torch.device('cuda')\n",
    "d_model=128\n",
    "n_heads=8\n",
    "lr = 1e-3\n",
    "betas=(.9,.98)\n",
    "epsilon=1e-9\n",
    "weight_decay=5e-4\n",
    "optimizer_params=(lr,betas,epsilon,weight_decay)\n",
    "Max_Epochs = 500\n",
    "Early_Stopping = 25\n",
    "dropout_rate=.3\n",
    "num_classes=18\n",
    "stride=1\n",
    "def compute_energy(x):\n",
    "    N, T, V, C = x.shape\n",
    "\n",
    "    x_values= x[:,:,:,0]\n",
    "    y_values = x[:, :, :, 1]\n",
    "    z_values = x[:, :, :, 2]\n",
    "    w=None\n",
    "    for v in range(V):\n",
    "        w_v=None\n",
    "        for t in range(1,T):\n",
    "            if w_v == None :\n",
    "                w_v = torch.sqrt(( x_values[:,t,v]/x_values[:,t-1,v] -1)**2 + ( y_values[:,t,v]/y_values[:,t-1,v] -1)**2 + ( z_values[:,t,v]/z_values[:,t-1,v] -1)**2)\n",
    "            else :\n",
    "                w_v  += torch.sqrt((x_values[:, t, v] / x_values[:, t - 1, v] - 1) ** 2 + (\n",
    "                            y_values[:, t, v] / y_values[:, t - 1, v] - 1) ** 2 + (\n",
    "                                           z_values[:, t, v] / z_values[:, t - 1, v] - 1) ** 2)\n",
    "        if w==None :\n",
    "            w=w_v\n",
    "        else :\n",
    "            w+=w_v\n",
    "    return w\n",
    "def init_data_loader():\n",
    "    train_loader, val_loader, test_loader, graph= load_data_sets(is_segmented=False)\n",
    "\n",
    "\n",
    "    return train_loader, val_loader, test_loader, graph\n",
    "\n",
    "\n",
    "def init_model(graph, optimizer_params, labels,num_classes,dropout_rate=.1):\n",
    "    model = STrGCN(graph, optimizer_params, labels, d_model=128,n_heads=8,num_classes=num_classes, dropout=dropout_rate)\n",
    "    \n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_acc(score, labels):\n",
    "    score = score.cpu().data.numpy()\n",
    "    labels = labels.cpu().data.numpy()\n",
    "    outputs = np.argmax(score, axis=1)\n",
    "    return np.sum(outputs == labels) / float(labels.size)\n",
    "\n",
    "def get_fp_rate(score,labels):\n",
    "    confusion_matrix=torchmetrics.ConfusionMatrix(num_classes=num_classes)\n",
    "\n",
    "\n",
    "    cnf_matrix = confusion_matrix(score.detach().cpu(), labels.detach().cpu())\n",
    "    FP = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
    "    FN = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix)\n",
    "    TP = np.diag(cnf_matrix)\n",
    "    TN = cnf_matrix.sum() - (FP + FN + TP)\n",
    "\n",
    "    FP = FP.type(torch.float)\n",
    "    TN = TN.type(torch.float)\n",
    "\n",
    "    # # Sensitivity, hit rate, recall, or true positive rate\n",
    "    # TPR = TP/(TP+FN)\n",
    "    # # Specificity or true negative rate\n",
    "    # TNR = TN/(TN+FP)\n",
    "    # # Precision or positive predictive value\n",
    "    # PPV = TP/(TP+FP)\n",
    "    # # Negative predictive value\n",
    "    # NPV = TN/(TN+FN)\n",
    "    # # Fall out or false positive rate\n",
    "    FPR = FP/(FP+TN)\n",
    "    # # False negative rate\n",
    "    # FNR = FN/(TP+FN)\n",
    "    # # False discovery rate\n",
    "    # FDR = FP/(TP+FP)\n",
    "    # # Overall accuracy\n",
    "    # ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "    return torch.sum(torch.nan_to_num(FPR),dim=-1)\n",
    "\n",
    "def get_window_label(label):\n",
    "    N,W=label.shape\n",
    "\n",
    "    sum=torch.zeros((1,num_classes))\n",
    "    for t in range(N):\n",
    "        sum[0,label[t]] += 1\n",
    "    out=sum.argmax(dim=-1)\n",
    "    return  out \n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n",
    "# fold for saving trained model...\n",
    "# change this path to the fold where you want to save your pre-trained model\n",
    "model_fold = \"./models/costr_gcn/online_model_checkpoints\"\n",
    "try:\n",
    "    os.mkdir(model_fold)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "train_loader, val_loader, test_loader, graph = init_data_loader()\n",
    "\n",
    "\n",
    "\n",
    "# .........inital model\n",
    "print(\"\\n loading model.............\")\n",
    "model = model = STrGCN.load_from_checkpoint(checkpoint_path=\"./models/STRGCN-SHREC17_2022-08-29_17_59_22/best_model-128-8-v1.ckpt\",adjacency_matrix=graph, optimizer_params=optimizer_params, labels=labels, d_model=128,n_heads=8,num_classes=num_classes, dropout=dropout_rate)\n",
    "# model_solver = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "\n",
    "# # ........set loss\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "\n",
    "# parameters recording training log\n",
    "\n",
    "\n",
    "f1_score=torchmetrics.F1Score(num_classes=num_classes)\n",
    "\n",
    "jaccard = torchmetrics.JaccardIndex(num_classes=num_classes)\n",
    "avg_precision = torchmetrics.AveragePrecision(num_classes=num_classes)\n",
    "eps=1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec39cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Testing **********\n",
      "here\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "#         # ***********evaluation***********\n",
    "print(\"*\"*10,\"Testing\",\"*\"*10)\n",
    "with torch.no_grad():\n",
    "    val_loss = 0\n",
    "    val_f1 = 0\n",
    "    val_jaccard=0\n",
    "    val_fp_rate=0\n",
    "    val_avg_precision=0\n",
    "    score_list = None\n",
    "    label_list = None\n",
    "    acc_sum = 0\n",
    "    # model.eval()\n",
    "    val_loss_epoch = 0\n",
    "    val_jaccard_epoch=0\n",
    "    val_fp_rate_epoch=0\n",
    "    val_avg_precision_epoch=0\n",
    "    val_f1_epoch = 0\n",
    "    for i, batch in enumerate(test_loader):\n",
    "        print(\"batch=\",i)\n",
    "        x,y,index=batch\n",
    "        y=torch.stack(y)\n",
    "        N, T, V, C = x.shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        score_list = None\n",
    "        label_list = None   \n",
    "        num_windows=T-window_size // window_size\n",
    "        for t in tqdm(range(0,T-window_size+1,stride), leave=False):\n",
    "            # print(i)\n",
    "            window=x[:,t:t+window_size]\n",
    "            label=get_window_label(y[t:t+window_size])\n",
    "            window = x[:,t: t+window_size].clone()\n",
    "            if t < 2*stride :\n",
    "                continue\n",
    "            window_i_m_2 = x[:,(t-2*stride): (t-2*stride)+window_size].clone()\n",
    "            window_i_m_1 = x[:,(t-1*stride):(t-1*stride)+window_size ].clone()\n",
    "            window_i = x[:,t: t+window_size].clone()\n",
    "            window_i_p_1 = x[:,t+1*stride: t+1*stride+window_size].clone()\n",
    "            window_i_p_2 = x[:,t+2*stride: (t+2*stride)+window_size].clone()\n",
    "\n",
    "            w_1=compute_energy(window_i_m_2)\n",
    "\n",
    "            w_2=compute_energy(window_i_m_1)\n",
    "            w_3=compute_energy(window_i)\n",
    "            w_4=compute_energy(window_i_p_1)\n",
    "            w_5=compute_energy(window_i_p_2)\n",
    "            d_wi=(w_4-w_2)/((t+1*stride)-(t-1*stride))\n",
    "            d_wi_m_1=(w_3-w_1)/(t-(t-2*stride))\n",
    "            d_wi_p_1=(w_5-w_3)/((t+2*stride)-t)\n",
    "            if d_wi < eps and d_wi_m_1 > 0 and d_wi_p_1 < 0 :\n",
    "                score = model(window)\n",
    "\n",
    "                if score_list is None:\n",
    "                    score_list = score\n",
    "                    label_list = label\n",
    "                else:\n",
    "                    score_list = torch.cat((score_list, score), 0)\n",
    "                    label_list = torch.cat((label_list, label), 0)\n",
    "\n",
    "\n",
    "        loss = criterion(score_list.detach().cpu(), label_list.detach().cpu())\n",
    "        score_list_labels= torch.argmax(torch.nn.functional.softmax(score_list, dim=-1), dim=-1)\n",
    "        val_f1_step= f1_score(score_list_labels.detach().cpu(), label_list.detach().cpu())\n",
    "        val_jaccard_step= jaccard(score_list_labels.detach().cpu(), label_list.detach().cpu())\n",
    "        val_fp_rate_step= get_fp_rate(score_list_labels.detach().cpu(), label_list.detach().cpu())\n",
    "        val_avg_precision_step=avg_precision(score_list.detach().cpu(), label_list.detach().cpu())\n",
    "        val_f1_epoch += val_f1_step\n",
    "        val_jaccard_epoch += val_jaccard_step\n",
    "        val_fp_rate_epoch += val_fp_rate_step\n",
    "        val_avg_precision_epoch+=val_avg_precision_step\n",
    "        val_loss += loss\n",
    "        print(\"*** SHREC  21\"\n",
    "            \"val_loss_step: %.6f,\"\n",
    "            \"val_F1_step: %.6f ***,\"\n",
    "            \"val_jaccard_step: %.6f ***\"\n",
    "            \"val_fp_rate_step: %.6f ***\"\n",
    "            \"val_avg_precision_step: %.6f ***\"\n",
    "            % ( loss, val_f1_step,val_jaccard_step, val_fp_rate_step,val_avg_precision_step))\n",
    "\n",
    "    val_loss = val_loss / (float(i + 1))\n",
    "    val_f1 = val_f1_epoch.item() / (float(i + 1))\n",
    "    val_jaccard = val_jaccard_epoch / (float(i + 1))\n",
    "    val_fp_rate = val_fp_rate_epoch / (float(i + 1))\n",
    "    val_avg_precision = val_avg_precision_epoch / (float(i + 1))\n",
    "    print(\"*** SHREC 21, \"\n",
    "            \"val_loss: %.6f,\"\n",
    "            \"val_F1: %.6f ***,\"\n",
    "            \"val_jaccard: %.6f ***\"\n",
    "            \"val_fp_rate: %.6f ***\"\n",
    "            \"val_avg_precision_rate: %.6f ***\"\n",
    "            % (val_loss, val_f1,val_jaccard, val_fp_rate, val_avg_precision))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
