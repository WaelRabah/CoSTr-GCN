{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0546779e",
   "metadata": {},
   "source": [
    "### Install modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f86aa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the cuda version if your GPU is compatible with newer versions\n",
    "!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
    "!pip install pytorch_lightning\n",
    "!pip install torchmetrics\n",
    "!pip install continual-inference\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07dcb88",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5d742038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "from model import CoSTrGCN\n",
    "from data_loaders.data_loader import load_data_sets\n",
    "import torchmetrics\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b50c14d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 108/108 [00:23<00:00,  4.55it/s]\n",
      "100%|██████████| 72/72 [00:12<00:00,  5.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data num:  72\n",
      "\n",
      " loading model.............\n"
     ]
    }
   ],
   "source": [
    "labels = [\n",
    "    \"NO GESTURE\",\n",
    "    \"RIGHT\",\n",
    "    \"KNOB\",\n",
    "    \"CROSS\",\n",
    "    \"THREE\",\n",
    "    \"V\",\n",
    "    \"ONE\",\n",
    "    \"FOUR\",\n",
    "    \"GRAB\",\n",
    "    \"DENY\",\n",
    "    \"MENU\",\n",
    "    \"CIRCLE\",\n",
    "    \"TAP\",\n",
    "    \"PINCH\",\n",
    "    \"LEFT\",\n",
    "    \"TWO\",\n",
    "    \"OK\",\n",
    "    \"EXPAND\",\n",
    "]\n",
    "DATASETS_PATH = \"datasets/\"\n",
    "DS_NAME = \"shrec21\"\n",
    "DS_PATH = DATASETS_PATH + \"shrec21/\"\n",
    "batch_size = 32\n",
    "workers = 4\n",
    "lr = 1e-4\n",
    "num_classes = 18\n",
    "window_size=10\n",
    "input_shape = (window_size,20,3)\n",
    "device = torch.device('cuda')\n",
    "d_model=128\n",
    "n_heads=8\n",
    "lr = 1e-3\n",
    "betas=(.9,.98)\n",
    "epsilon=1e-9\n",
    "weight_decay=5e-4\n",
    "optimizer_params=(lr,betas,epsilon,weight_decay)\n",
    "Max_Epochs = 500\n",
    "Early_Stopping = 25\n",
    "dropout_rate=.3\n",
    "num_classes=18\n",
    "stride=1\n",
    "def compute_energy(x):\n",
    "    N, T, V, C = x.shape\n",
    "\n",
    "    x_values= x[:,:,:,0]\n",
    "    y_values = x[:, :, :, 1]\n",
    "    z_values = x[:, :, :, 2]\n",
    "    w=None\n",
    "    for v in range(V):\n",
    "        w_v=None\n",
    "        for t in range(1,T):\n",
    "            if w_v == None :\n",
    "                w_v = torch.sqrt(( x_values[:,t,v]/x_values[:,t-1,v] -1)**2 + ( y_values[:,t,v]/y_values[:,t-1,v] -1)**2 + ( z_values[:,t,v]/z_values[:,t-1,v] -1)**2)\n",
    "            else :\n",
    "                w_v  += torch.sqrt((x_values[:, t, v] / x_values[:, t - 1, v] - 1) ** 2 + (\n",
    "                            y_values[:, t, v] / y_values[:, t - 1, v] - 1) ** 2 + (\n",
    "                                           z_values[:, t, v] / z_values[:, t - 1, v] - 1) ** 2)\n",
    "        if w==None :\n",
    "            w=w_v\n",
    "        else :\n",
    "            w+=w_v\n",
    "    return w\n",
    "def init_data_loader():\n",
    "    train_loader, val_loader, test_loader, graph = load_data_sets(\n",
    "    window_size=window_size,\n",
    "        batch_size=batch_size,\n",
    "        workers=workers,\n",
    "        is_segmented=False,\n",
    "        binary_classes=False,\n",
    "        use_data_aug=False,\n",
    "        use_aug_by_sw=False\n",
    "        )\n",
    "\n",
    "    return train_loader, val_loader, test_loader, graph\n",
    "\n",
    "\n",
    "def init_model(graph, optimizer_params, labels,num_classes,dropout_rate=.1):\n",
    "    model = CoSTrGCN(graph, optimizer_params, labels, d_model=128,n_heads=8,num_classes=num_classes, dropout=dropout_rate)\n",
    "    return model\n",
    "\n",
    "def get_acc(score, labels):\n",
    "    score = score.cpu().data.numpy()\n",
    "    labels = labels.cpu().data.numpy()\n",
    "    outputs = np.argmax(score, axis=1)\n",
    "    return np.sum(outputs == labels) / float(labels.size)\n",
    "\n",
    "def get_fp_rate(score,labels):\n",
    "    confusion_matrix=torchmetrics.StatScores(num_classes=num_classes,reduce=\"micro\")\n",
    "\n",
    "\n",
    "    TP, FP, TN, FN, SUP = confusion_matrix(score, labels)\n",
    "    # FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix)\n",
    "    # FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
    "    # TP = np.diag(cnf_matrix)\n",
    "    # TN = cnf_matrix.sum() - (FP + FN + TP)\n",
    "\n",
    "    FP = FP.type(torch.float)\n",
    "    TN = TN.type(torch.float)\n",
    "\n",
    "    # # Sensitivity, hit rate, recall, or true positive rate\n",
    "    # TPR = TP/(TP+FN)\n",
    "    # # Specificity or true negative rate\n",
    "    # TNR = TN/(TN+FP)\n",
    "    # # Precision or positive predictive value\n",
    "    # PPV = TP/(TP+FP)\n",
    "    # # Negative predictive value\n",
    "    # NPV = TN/(TN+FN)\n",
    "    # # Fall out or false positive rate\n",
    "    FPR = FP/(FP+TN)\n",
    "    # # False negative rate\n",
    "    # FNR = FN/(TP+FN)\n",
    "    # # False discovery rate\n",
    "    # FDR = FP/(TP+FP)\n",
    "    # # Overall accuracy\n",
    "    # ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "    return torch.sum(torch.nan_to_num(FPR),dim=-1)\n",
    "\n",
    "def get_window_label(label):\n",
    "    N,W=label.shape\n",
    "\n",
    "    sum=torch.zeros((1,num_classes))\n",
    "    for t in range(N):\n",
    "        sum[0,label[t]] += 1\n",
    "    out=sum.argmax(dim=-1)\n",
    "    return  out \n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n",
    "# fold for saving trained model...\n",
    "# change this path to the fold where you want to save your pre-trained model\n",
    "model_fold = \"./models/costr_gcn/online_model_checkpoints\"\n",
    "try:\n",
    "    os.mkdir(model_fold)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "train_loader, val_loader, test_loader, graph = init_data_loader()\n",
    "\n",
    "\n",
    "\n",
    "# .........inital model\n",
    "print(\"\\n loading model.............\")\n",
    "model = model = CoSTrGCN.load_from_checkpoint(checkpoint_path=\"./models/CoSTrGCN-SHREC21_2022-09-10_14_32_38/best_model-128-8-v1.ckpt\",adjacency_matrix=graph, optimizer_params=optimizer_params, labels=labels, d_model=128,n_heads=8,num_classes=num_classes, dropout=dropout_rate)\n",
    "# model_solver = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "\n",
    "# # ........set loss\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# # ........Metrics definition\n",
    "f1_score=torchmetrics.F1Score(num_classes=num_classes)\n",
    "jaccard = torchmetrics.JaccardIndex(num_classes=num_classes)\n",
    "avg_precision = torchmetrics.AveragePrecision(num_classes=num_classes)\n",
    "eps=1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a3f90e6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CoSTrGCN(\n",
       "  (loss): CrossEntropyLoss()\n",
       "  (train_acc): Accuracy()\n",
       "  (valid_acc): Accuracy()\n",
       "  (test_acc): Accuracy()\n",
       "  (confusion_matrix): ConfusionMatrix()\n",
       "  (gcn): SGCN(\n",
       "    (conv_layers): ModuleList(\n",
       "      (0): unit_gcn(\n",
       "        (conv_list): ModuleList(\n",
       "          (0): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (2): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): Mish()\n",
       "      )\n",
       "      (1): unit_gcn(\n",
       "        (conv_list): ModuleList(\n",
       "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): Mish()\n",
       "      )\n",
       "      (2): unit_gcn(\n",
       "        (conv_list): ModuleList(\n",
       "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): Mish()\n",
       "      )\n",
       "      (3): unit_gcn(\n",
       "        (conv_list): ModuleList(\n",
       "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): Mish()\n",
       "      )\n",
       "      (4): unit_gcn(\n",
       "        (conv_list): ModuleList(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (2): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): Mish()\n",
       "      )\n",
       "      (5): unit_gcn(\n",
       "        (conv_list): ModuleList(\n",
       "          (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): Mish()\n",
       "      )\n",
       "      (6): unit_gcn(\n",
       "        (conv_list): ModuleList(\n",
       "          (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): Mish()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (encoder): TransformerGraphEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerGraphEncoderLayer(\n",
       "        (attention): Residual(\n",
       "          (sublayer): MultiHeadAttention(\n",
       "            (heads): ModuleList(\n",
       "              (0): AttentionHead(\n",
       "                (q_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (k_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (v_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (1): AttentionHead(\n",
       "                (q_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (k_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (v_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (2): AttentionHead(\n",
       "                (q_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (k_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (v_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (3): AttentionHead(\n",
       "                (q_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (k_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (v_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (4): AttentionHead(\n",
       "                (q_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (k_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (v_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (5): AttentionHead(\n",
       "                (q_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (k_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (v_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (6): AttentionHead(\n",
       "                (q_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (k_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (v_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (7): AttentionHead(\n",
       "                (q_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (k_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (v_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "            )\n",
       "            (linear): Linear(in_features=256, out_features=128, bias=True)\n",
       "          )\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (feed_forward): Residual(\n",
       "          (sublayer): FeedForward(\n",
       "            (out): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (1): Mish()\n",
       "              (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): TransformerGraphEncoderLayer(\n",
       "        (attention): Residual(\n",
       "          (sublayer): MultiHeadAttention(\n",
       "            (heads): ModuleList(\n",
       "              (0): AttentionHead(\n",
       "                (q_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (k_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (v_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (1): AttentionHead(\n",
       "                (q_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (k_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (v_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (2): AttentionHead(\n",
       "                (q_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (k_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (v_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (3): AttentionHead(\n",
       "                (q_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (k_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (v_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (4): AttentionHead(\n",
       "                (q_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (k_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (v_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (5): AttentionHead(\n",
       "                (q_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (k_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (v_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (6): AttentionHead(\n",
       "                (q_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (k_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (v_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (7): AttentionHead(\n",
       "                (q_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (k_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (v_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "            )\n",
       "            (linear): Linear(in_features=256, out_features=128, bias=True)\n",
       "          )\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (feed_forward): Residual(\n",
       "          (sublayer): FeedForward(\n",
       "            (out): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (1): Mish()\n",
       "              (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): TransformerGraphEncoderLayer(\n",
       "        (attention): Residual(\n",
       "          (sublayer): MultiHeadAttention(\n",
       "            (heads): ModuleList(\n",
       "              (0): AttentionHead(\n",
       "                (q_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (k_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (v_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (1): AttentionHead(\n",
       "                (q_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (k_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (v_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (2): AttentionHead(\n",
       "                (q_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (k_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (v_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (3): AttentionHead(\n",
       "                (q_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (k_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (v_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (4): AttentionHead(\n",
       "                (q_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (k_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (v_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (5): AttentionHead(\n",
       "                (q_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (k_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (v_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (6): AttentionHead(\n",
       "                (q_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (k_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (v_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (7): AttentionHead(\n",
       "                (q_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (k_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (v_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "            )\n",
       "            (linear): Linear(in_features=256, out_features=128, bias=True)\n",
       "          )\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (feed_forward): Residual(\n",
       "          (sublayer): FeedForward(\n",
       "            (out): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (1): Mish()\n",
       "              (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): TransformerGraphEncoderLayer(\n",
       "        (attention): Residual(\n",
       "          (sublayer): MultiHeadAttention(\n",
       "            (heads): ModuleList(\n",
       "              (0): AttentionHead(\n",
       "                (q_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (k_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (v_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (1): AttentionHead(\n",
       "                (q_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (k_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (v_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (2): AttentionHead(\n",
       "                (q_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (k_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (v_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (3): AttentionHead(\n",
       "                (q_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (k_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (v_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (4): AttentionHead(\n",
       "                (q_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (k_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (v_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (5): AttentionHead(\n",
       "                (q_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (k_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (v_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (6): AttentionHead(\n",
       "                (q_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (k_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (v_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (7): AttentionHead(\n",
       "                (q_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (k_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (v_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "            )\n",
       "            (linear): Linear(in_features=256, out_features=128, bias=True)\n",
       "          )\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (feed_forward): Residual(\n",
       "          (sublayer): FeedForward(\n",
       "            (out): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (1): Mish()\n",
       "              (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): TransformerGraphEncoderLayer(\n",
       "        (attention): Residual(\n",
       "          (sublayer): MultiHeadAttention(\n",
       "            (heads): ModuleList(\n",
       "              (0): AttentionHead(\n",
       "                (q_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (k_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (v_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (1): AttentionHead(\n",
       "                (q_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (k_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (v_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (2): AttentionHead(\n",
       "                (q_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (k_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (v_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (3): AttentionHead(\n",
       "                (q_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (k_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (v_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (4): AttentionHead(\n",
       "                (q_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (k_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (v_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (5): AttentionHead(\n",
       "                (q_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (k_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (v_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (6): AttentionHead(\n",
       "                (q_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (k_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (v_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (7): AttentionHead(\n",
       "                (q_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (k_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (v_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "            )\n",
       "            (linear): Linear(in_features=256, out_features=128, bias=True)\n",
       "          )\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (feed_forward): Residual(\n",
       "          (sublayer): FeedForward(\n",
       "            (out): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (1): Mish()\n",
       "              (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): TransformerGraphEncoderLayer(\n",
       "        (attention): Residual(\n",
       "          (sublayer): MultiHeadAttention(\n",
       "            (heads): ModuleList(\n",
       "              (0): AttentionHead(\n",
       "                (q_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (k_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (v_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (1): AttentionHead(\n",
       "                (q_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (k_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (v_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (2): AttentionHead(\n",
       "                (q_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (k_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (v_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (3): AttentionHead(\n",
       "                (q_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (k_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (v_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (4): AttentionHead(\n",
       "                (q_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (k_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (v_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (5): AttentionHead(\n",
       "                (q_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (k_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (v_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (6): AttentionHead(\n",
       "                (q_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (k_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (v_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (7): AttentionHead(\n",
       "                (q_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (k_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (v_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "            )\n",
       "            (linear): Linear(in_features=256, out_features=128, bias=True)\n",
       "          )\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (feed_forward): Residual(\n",
       "          (sublayer): FeedForward(\n",
       "            (out): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (1): Mish()\n",
       "              (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (positional_encoder): PositionalEncoder(\n",
       "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (out): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (1): Mish()\n",
       "    (2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (3): Linear(in_features=128, out_features=18, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2ae2daf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_romannian_mean(w):\n",
    "    N,T,V,C=w.shape\n",
    "    mu=w[:,:,0]\n",
    "    old_mu=torch.clone(mu)\n",
    "    num_el=torch.numel(mu)\n",
    "    while True :\n",
    "        s=None\n",
    "        dis=lambda mu_,p: torch.norm(torch.log(p) / torch.log(mu_),dim=-1)\n",
    "        for v in range(1,V) :\n",
    "            if s==None :\n",
    "                s=dis(mu,w[:,:,v])**2\n",
    "            else :\n",
    "                s+=dis(mu,w[:,:,v])**2\n",
    "        old_mu=torch.clone(mu)\n",
    "        mu=w[:,torch.argmin(s)]\n",
    "        if torch.sum(mu==old_mu)==num_el:\n",
    "            break\n",
    "    return mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "de562f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch= 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (30) must match the size of tensor b (20) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\WR11\\Desktop\\codebases\\CoSTr-GCN\\Online_testing.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/WR11/Desktop/codebases/CoSTr-GCN/Online_testing.ipynb#X30sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# print(y[240:290])\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/WR11/Desktop/codebases/CoSTr-GCN/Online_testing.ipynb#X30sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m y\u001b[39m=\u001b[39my[\u001b[39m270\u001b[39m:\u001b[39m290\u001b[39m]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/WR11/Desktop/codebases/CoSTr-GCN/Online_testing.ipynb#X30sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m mu\u001b[39m=\u001b[39mcompute_romannian_mean(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/WR11/Desktop/codebases/CoSTr-GCN/Online_testing.ipynb#X30sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(mu\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/WR11/Desktop/codebases/CoSTr-GCN/Online_testing.ipynb#X30sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# if i==0:\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/WR11/Desktop/codebases/CoSTr-GCN/Online_testing.ipynb#X30sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m#     break\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/WR11/Desktop/codebases/CoSTr-GCN/Online_testing.ipynb#X30sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# score = model(x)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/WR11/Desktop/codebases/CoSTr-GCN/Online_testing.ipynb#X30sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# print(label)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/WR11/Desktop/codebases/CoSTr-GCN/Online_testing.ipynb#X30sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# print(score_list_labels)\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\WR11\\Desktop\\codebases\\CoSTr-GCN\\Online_testing.ipynb Cell 8\u001b[0m in \u001b[0;36mcompute_romannian_mean\u001b[1;34m(w)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/WR11/Desktop/codebases/CoSTr-GCN/Online_testing.ipynb#X30sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m,V) :\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/WR11/Desktop/codebases/CoSTr-GCN/Online_testing.ipynb#X30sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mif\u001b[39;00m s\u001b[39m==\u001b[39m\u001b[39mNone\u001b[39;00m :\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/WR11/Desktop/codebases/CoSTr-GCN/Online_testing.ipynb#X30sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         s\u001b[39m=\u001b[39mdis(mu,w[:,:,v])\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/WR11/Desktop/codebases/CoSTr-GCN/Online_testing.ipynb#X30sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39melse\u001b[39;00m :\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/WR11/Desktop/codebases/CoSTr-GCN/Online_testing.ipynb#X30sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         s\u001b[39m+\u001b[39m\u001b[39m=\u001b[39mdis(mu,w[:,:,v])\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m\n",
      "\u001b[1;32mc:\\Users\\WR11\\Desktop\\codebases\\CoSTr-GCN\\Online_testing.ipynb Cell 8\u001b[0m in \u001b[0;36mcompute_romannian_mean.<locals>.<lambda>\u001b[1;34m(mu_, p)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/WR11/Desktop/codebases/CoSTr-GCN/Online_testing.ipynb#X30sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m :\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/WR11/Desktop/codebases/CoSTr-GCN/Online_testing.ipynb#X30sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     s\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/WR11/Desktop/codebases/CoSTr-GCN/Online_testing.ipynb#X30sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     dis\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m mu_,p: torch\u001b[39m.\u001b[39mnorm(torch\u001b[39m.\u001b[39;49mlog(p) \u001b[39m/\u001b[39;49m torch\u001b[39m.\u001b[39;49mlog(mu_),dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/WR11/Desktop/codebases/CoSTr-GCN/Online_testing.ipynb#X30sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m,V) :\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/WR11/Desktop/codebases/CoSTr-GCN/Online_testing.ipynb#X30sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         \u001b[39mif\u001b[39;00m s\u001b[39m==\u001b[39m\u001b[39mNone\u001b[39;00m :\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (30) must match the size of tensor b (20) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(test_loader):\n",
    "    print(\"batch=\",i)\n",
    "\n",
    "    x,y,index=batch\n",
    "    x=x[:,270:300]\n",
    "    y=torch.stack(y)\n",
    "    # print(y[240:290])\n",
    "    y=y[270:290]\n",
    "\n",
    "    mu=compute_romannian_mean(x)\n",
    "    print(mu.shape)\n",
    "    # if i==0:\n",
    "    #     break\n",
    "    # score = model(x)\n",
    "    # score=torch.cat([score,torch.tensor([[10.]],dtype=torch.float).cuda()],dim=-1)\n",
    "    # print(score)\n",
    "    # label=get_window_label(y)\n",
    "    # prob=torch.nn.functional.softmax(score, dim=-1)\n",
    "    # score_list_labels= torch.argmax(prob, dim=-1)\n",
    "    # print(prob)\n",
    "    # print(label)\n",
    "    # print(score_list_labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be538364",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(test_loader):\n",
    "    print(\"batch=\",i)\n",
    "    if i>0:\n",
    "        break\n",
    "    x,y,index=batch\n",
    "    x=x[:,50:220]\n",
    "    N, T, V, C = x.shape\n",
    "    y=torch.stack(y)\n",
    "    y=y[50:220]\n",
    "    window_size=50\n",
    "    for t in range(0,T-window_size+1,window_size):\n",
    "        \n",
    "        window=x[:,t:t+window_size]\n",
    "        \n",
    "\n",
    "        label_l=y[t:t+window_size]\n",
    "        label=get_window_label(label_l)\n",
    "        score = model(window)\n",
    "        print(score)\n",
    "        # label=get_window_label(y)\n",
    "        prob=torch.nn.functional.softmax(score, dim=-1)\n",
    "        score_list_labels= torch.argmax(prob, dim=-1)\n",
    "        print(prob)\n",
    "        print(label)\n",
    "        print(score_list_labels)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec39cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "#         # ***********evaluation***********\n",
    "print(\"*\"*10,\"Testing\",\"*\"*10)\n",
    "with torch.no_grad():\n",
    "    val_loss = 0\n",
    "    val_f1 = 0\n",
    "    val_jaccard=0\n",
    "    val_fp_rate=0\n",
    "    val_avg_precision=0\n",
    "    score_list = None\n",
    "    label_list = None\n",
    "    acc_sum = 0\n",
    "    # model.eval()\n",
    "    val_loss_epoch = 0\n",
    "    val_jaccard_epoch=0\n",
    "    val_fp_rate_epoch=0\n",
    "    val_avg_precision_epoch=0\n",
    "    val_f1_epoch = 0\n",
    "    for i, batch in enumerate(test_loader):\n",
    "        print(\"batch=\",i)\n",
    "        x,y,index=batch\n",
    "        y=torch.stack(y)\n",
    "        N, T, V, C = x.shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        score_list = None\n",
    "        label_list = None   \n",
    "        num_windows=T-window_size // window_size\n",
    "        for t in tqdm(range(0,T-window_size+1,stride), leave=False):\n",
    "            # print(i)\n",
    "            window=x[:,t:t+window_size]\n",
    "            label=get_window_label(y[t:t+window_size])\n",
    "            window = x[:,t: t+window_size].clone()\n",
    "            if t < 2*stride :\n",
    "                continue\n",
    "            window_i_m_2 = x[:,(t-2*stride): (t-2*stride)+window_size].clone()\n",
    "            window_i_m_1 = x[:,(t-1*stride):(t-1*stride)+window_size ].clone()\n",
    "            window_i = x[:,t: t+window_size].clone()\n",
    "            window_i_p_1 = x[:,t+1*stride: t+1*stride+window_size].clone()\n",
    "            window_i_p_2 = x[:,t+2*stride: (t+2*stride)+window_size].clone()\n",
    "\n",
    "            w_1=compute_energy(window_i_m_2)\n",
    "\n",
    "            w_2=compute_energy(window_i_m_1)\n",
    "            w_3=compute_energy(window_i)\n",
    "            w_4=compute_energy(window_i_p_1)\n",
    "            w_5=compute_energy(window_i_p_2)\n",
    "            d_wi=(w_4-w_2)/((t+1*stride)-(t-1*stride))\n",
    "            d_wi_m_1=(w_3-w_1)/(t-(t-2*stride))\n",
    "            d_wi_p_1=(w_5-w_3)/((t+2*stride)-t)\n",
    "            if d_wi < eps and d_wi_m_1 > 0 and d_wi_p_1 < 0 :\n",
    "                score = model(window)\n",
    "\n",
    "                if score_list is None:\n",
    "                    score_list = score\n",
    "                    label_list = label\n",
    "                else:\n",
    "                    score_list = torch.cat((score_list, score), 0)\n",
    "                    label_list = torch.cat((label_list, label), 0)\n",
    "\n",
    "\n",
    "        loss = criterion(score_list.detach().cpu(), label_list.detach().cpu())\n",
    "        score_list_labels= torch.argmax(torch.nn.functional.softmax(score_list, dim=-1), dim=-1)\n",
    "        val_f1_step= f1_score(score_list_labels.detach().cpu(), label_list.detach().cpu())\n",
    "        val_jaccard_step= jaccard(score_list_labels.detach().cpu(), label_list.detach().cpu())\n",
    "        val_fp_rate_step= get_fp_rate(score_list_labels.detach().cpu(), label_list.detach().cpu())\n",
    "        val_avg_precision_step=avg_precision(score_list.detach().cpu(), label_list.detach().cpu())\n",
    "        val_f1_epoch += val_f1_step\n",
    "        val_jaccard_epoch += val_jaccard_step\n",
    "        val_fp_rate_epoch += val_fp_rate_step\n",
    "        val_avg_precision_epoch+=val_avg_precision_step\n",
    "        val_loss += loss\n",
    "        print(\"*** SHREC  21\"\n",
    "            \"val_loss_step: %.6f,\"\n",
    "            \"val_F1_step: %.6f ***,\"\n",
    "            \"val_jaccard_step: %.6f ***\"\n",
    "            \"val_fp_rate_step: %.6f ***\"\n",
    "            \"val_avg_precision_step: %.6f ***\"\n",
    "            % ( loss, val_f1_step,val_jaccard_step, val_fp_rate_step,val_avg_precision_step))\n",
    "\n",
    "    val_loss = val_loss / (float(i + 1))\n",
    "    val_f1 = val_f1_epoch.item() / (float(i + 1))\n",
    "    val_jaccard = val_jaccard_epoch / (float(i + 1))\n",
    "    val_fp_rate = val_fp_rate_epoch / (float(i + 1))\n",
    "    val_avg_precision = val_avg_precision_epoch / (float(i + 1))\n",
    "    print(\"*** SHREC 21, \"\n",
    "            \"val_loss: %.6f,\"\n",
    "            \"val_F1: %.6f ***,\"\n",
    "            \"val_jaccard: %.6f ***\"\n",
    "            \"val_fp_rate: %.6f ***\"\n",
    "            \"val_avg_precision_rate: %.6f ***\"\n",
    "            % (val_loss, val_f1,val_jaccard, val_fp_rate, val_avg_precision))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "0185029bc7e5ab5fd6a37d237d2b054f8252c7c82f5be991d443c7f2d7ddb2f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
